%!TEX root = ../thesis.tex

\begin{vcenterpage}
% \noindent\rule[2pt]{\textwidth}{0.5pt}
% \begin{center}
% {\large\textbf{\thesistitle\\}}
% \end{center}
\begin{center}
{\LARGE\textbf{Abstract}} 
\end{center}

% \begin{center}
% \noindent\rule[2pt]{0.2\textwidth}{0.5pt}
% \end{center}
% \vspace{1cm}

This thesis investigates how a machine can be taught a new task from unlabeled human instructions, which is without knowing beforehand how to associate the human communicative signals with their meanings. The theoretical and empirical work presented in this thesis provide means to create calibration free interactive systems, which allow humans to interact with machines, from scratch, using their own preferred teaching signals. It therefore removes the need for an expert to tune the system for each specific user, which constitute an important step towards flexible personalized teaching interfaces, a key for the future of personal robotics.
% This thesis investigates how a robot can be taught what to do from instructions provided by a human without knowing beforehand how to associate the human communicative signals to their meanings.

Our approach assumes the robot has access to a limited set of task hypotheses, which include the task the user wants to solve. Our method consists of generating interpretation hypotheses of the teaching signals with respect to each hypothetic task. By building a set of hypothetic interpretation, i.e. a set of signal-label pairs for each task, the task the user wants to solve is the one that explains better the history of interaction.

% For this work, we consider different scenarios where a human teacher uses initially unclassified speech words, whose associated meaning can be a feedback (correct/incorrect) or a guidance (go left, right, up, \ldots). We consider different scenarios where a human teaches a robot to perform a new task using initially unclassified signals, whose associated meaning can be a feedback (correct/incorrect) or a guidance (go left, right, up, \ldots).

We consider different scenarios, including a pick and place robotics experiment with speech as the modality of interaction, and a navigation task in a brain computer interaction scenario. In these scenario, a teacher instructs a robot to perform a new task using initially unclassified signals, whose associated meaning can be a feedback (correct/incorrect) or a guidance (go left, right, up, \ldots). Our results show that a) it is possible to learn the meaning of unlabeled and noisy teaching signals, as well as a new task at the same time, and b) it is possible to reuse the acquired knowledge about the teaching signals for learning new tasks faster. We further introduce a planning strategy that exploits uncertainty from the task and the signals' meanings to allow more efficient learning sessions. We present a study where several real human subjects control successfully a virtual device using their brain and without relying on a calibration phase. Our system identifies, from scratch, the target intended by the user as well as the decoder of brain signals.



Based on this work, but from an other perspective, we introduce a new experimental setup to study how human behave in unconstrained asymmetric collaborative task. In this setup, two humans have to collaborate to solve a task but the channels of communication they can use are constrained and force them to invent and agree on a shared interaction protocol in order to solve the task. These constraints allow to analyze how a communication protocol is progressively established through the interplay and history of individual actions.

% Finally, we address a number of limitations and propose extensions to the problems of continuous state space, continuous task space, and unknown interaction protocols. 





% {\large\textbf{Keywords:}} Keyword 1, Keyword2.\\

% \noindent\rule[2pt]{\textwidth}{0.5pt}
\end{vcenterpage}