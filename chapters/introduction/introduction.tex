%!TEX root = ../../thesis.tex
\define{\chapterpath}{\allchapterspath/introduction}
\define{\imgpath}{\chapterpath/img}

\chapter{Introduction}
\label{chapter:introduction}
\minitoc

This thesis investigates how a robot can be taught what to do from instructions provided by a human without knowing beforehand how to associate the human communicative signals to their meanings.

In the past decades, robotics and autonomous systems have seen tremendous improvement in their motor, perceptual, and computational capabilities. As a good example, we have been able to send and operate rovers for several years on the planet Mars (Spirit, Opportunity, Curiosity), which indicates the technologies are well mastered. However, getting such robots to do what we want them to do remains a skill of few, and bringing robotics system teachable by everyone and capable of social interaction in our daily life has been identified as the next milestone for the robotic community.

As for bringing computers in all our homes required easy and intuitive ways for people to make use of them, bringing robots in our daily life requires easy and intuitive ways for people to make robots do useful things for them. Currently, implementing advance behaviors in a robot, such as folding a shirt, requires similar skills than a computer scientist needed 50 years ago to build any application using punch cards, i.e. a lot of expertise, patience and trials and errors development. Due to the diversity of skills a robot should be able to execute in our daily environment, including interacting with humans and objects, traditional programming methods hinder the deployment of robotic system at homes and workspaces.

Instead, researchers are trying to endow robotic systems with the ability to learn from social interaction what tasks to execute and how they should be executed. Several methods have been considered to allow non-technical users to ``program'' robots, such as \emph{learning by demonstration} where the human demonstrate the skills to the robot, \emph{learning from reinforcement} where the human assesses the actions of the robot with respect to the aimed behavior, or \emph{learning from instructions} where the human explains the sequence of actions to performs in order to fulfill the task . Advances in these areas should allow the emergence of robot companions, living inside our homes providing assistance to the daily tasks, care, and entertainment. 

Endowing a robot with the ability to learn form interaction with a human partners requires to solve several challenges: the technical challenge of motor, perceptual and cognitive skills acquisition and generalization, the practical challenge of interacting in a social way with human being of different background. Especially, the robot must be able to understand the communicative signals from the human and communicate its own intention and ``state of mind'' back to the human. Additionally, as robots are embodied agent, the challenge of social acceptance of robot among people is an additional obstacle. A robot that is not accepted by people will not be used, and a robot that cannot be understood by people or a non-intuitive interface is likely to impact the performances of the learning systems developed.

Currently most of the challenges are considered in isolation. For example, when a robot learns a task from human instructions, it is assumed the robot receives instructions in a symbolic way, e.g. if the human uses speech to communicate his instructions the robot is assumed to be able convert raw speech into text. Similarly, when a robot learns how to recognize speech utterances, which is how to convert raw speech into a meaningful representation such as text, the robot is usually fed with many examples of speech utterances associated  with their symbolic representation.

In this thesis, we consider the two latter challenges simultaneously which is learning a new task from raw human instructions signals whose associated meanings are initially unknown. Solving this problem would allow the same robot to be taught by a variety of users using their preferred teaching signals and without the intervention of an expert to calibrate the system to the specific teaching signals used by each users. For example, a robot that accepts speech commands usually accept only one or a limited set of pre-specified speech utterances for each command, e.g. using the word ``forward'' to ask the robot to move forward. With the method described in this thesis, the user could use its preferred word to ask the robot to move forward, e.g. ``straight'' or ``up'', but also words whose usual meanings are non-related to the move forward action such as ``dog'', ``backwards'', or ``blue'', or interjection such as ``ah'', ``oh'', or even non speech utterance such as a hand clapping. The robot, after some practical interaction with the user, will find out which signal is associated to the action moving forward.

Our approach assumes the robot has access to a limited set of task hypothesis which include the task the user wants to solve. For example, if the user's goal is to guide the robot towards a specific room in a house, there is a limited number of possible tasks which are the number of rooms in the house. The idea behind our method consists of generating interpretation hypothesis of the teaching signals with respect to each hypothetic task. We will see that by building a set of hypothetic interpretation, i.e. a set of signal-label pairs for each task, the task the user wants to solve stands out by having the best coherence between the underlying spacial organization of the signals in their feature space and the labels associated to each signal. In others words, the correct task is the one that explains better the history of interaction.

In the following this introduction, we present in more details the challenges of learning from social interaction with humans and explicit the usual assumptions made when designing such systems. On this basis, we define the specific challenges addressed in this work and describe the contribution of the thesis.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Social Learning: Robot learning from interaction with humans}
\label{sec:intro:social}

It is often easier to acquire a new skill if someone that already acquired that skill teach us how to do it. The field of social learning in robotics investigates how knowledge can be transferred from humans to robots through social interaction. Social interaction implies the human interacts with the machine using similar modalities as when interacting with other human beings, for example using speech, gestures, or by demonstrating some behaviors. 

We can identify three main social learning paradigms used in robotics today: \begin{inparaenum}[(a)] \item learning from human demonstration, where the robot learns by imitating the human actions, \item learning from human reinforcement, where the robot learn from assessments on its own actions provided by the user, and \item learning from human instructions, where the robot learns from concrete instructions about what do to next provided by the user. \end{inparaenum}

Each of those paradigms requires to solve two main challenges: \begin{inparaenum}[(1)] \item the robot should be able to identify which parts of the interaction, and of the environment, are relevant to the acquisition of the new skill, and \item the robot should be able to infer, from the relevant informations extracted from the interaction, the new skill or task the human wants the robot to achieve. \end{inparaenum}

As we will see in the following subsections, most of the work in robot social learning considered those two latter challenges separately and most of the efforts focused on the second challenge of designing better learning algorithm.

% This approach contrast with the usual methods used to endow a robot with a new skill, such as directly programming the robot or defining a mathematical fitness function for that skill. 
% Social interaction, by being the most intuitive way for humans to communicate,   are the most intuitive way human communicative between each others and if robot are able
% By using speech, gestures, or by providing demonstration, human users can teach a robot a new skill without knowing how to program or how to define a fitness function this a particular skill. 
% This way human users can interact with machines without knowing how to program or how to define a fitness function for a particular task. People will be more familiar  of the desired skill, which correspond to more intuitive interaction for humans.

% In the following of this section we present three main social learning paradigms: \begin{inparaenum}[(a)] \item learning from human demonstration, where the robot learns by imitating the human actions, \item learning from human reinforcement, where the robot learn from assessments on its own actions provided by the user, and \item learning from human instructions, where the robot learns from concrete instructions about what do to next provided by the user. \end{inparaenum}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Learning from human demonstrations}

Learning from human demonstrations, also called programming by demonstration or learning by imitation, is the process of learning a new skill from practical examples of how to perform a this skill \cite{schaal1999imitation,calinon2008robot,argall09survey,lopes10imitationchapter}. More formally, the robot should infer a policy which is a mapping between world states and actions by observing only a  some examples of state to action mapping, i.e. some demonstrations, which may be noisy and incomplete.

Following the survey of Brenna D. Argall \cite{argall09survey}, we will segment our presentation of learning from human demonstration in three parts, first we will present the different method used to collect training data, i..e to gather the demonstration, then we will present several methods allowing to derive a policy from demonstration, and finally we highlight some limitations of the method.

% Demonstrating a task to a robot is easier for the user than directly programming a behavior by writing lines of code. But creating a robot that learn from example requires to solves two main problems: \begin{inparaenum}[(1)] \item creating algorithm capable of inferring the user desired behavior from its demonstrations, but also \item allowing the robot to identify what part of the environment and of the human behavior are relevant to the task, i.e. to extract the information related to a demonstration from the interaction. \end{inparaenum}

\subsubsection*{Collecting demonstrations}

Collecting demonstration is probably the most important part in the learning process. Demonstration of good qualities will results in an easier learning while bad quality demonstrations are likely to impact the quality of the learned behavior.

We group the demonstration recording methods in two categories: \begin{inparaenum} \item teleoperation where the human demonstrate the skill by directly control the robot, and \item external observation where the robot is observing the human providing demonstration. \end{inparaenum} More formally, learning from data collected by a direct control of the robot by the human is called learning from demonstration, while learning from data collected by observing the human demonstrating the skill is called learning from imitation. We will see that this latter approach leverage some embodiment issues.

During teleoperation, the robot is directly operated by the teacher and therefore record the demonstration using it own sensors, i.e. the robot directly observe a sequence of state-action pairs in his own referential. It is the most direct and most efficient method to provide demonstration. However this method does not always apply well to all robots, for example, robot with many degrees of freedom can not be teleoperated efficiently by one person, but also robots that should maintain equilibrium are sometimes impossible to manipulate directly, such as demonstrating a walking behavior by teleoperating the legs of a humanoid robot.

Teleoperation has been used in a variety of robotic applications, including learning of aerobatic helicopter flight \cite{abbeel2007application}, object displacement with environmental constrained (e.g. obstacle avoidance) \cite{guenter2007reinforcement,calinon2007teacher}, object stacking \cite{calinon2007teacher}, or ball grasping on the Aibo robot \cite{grollman2007learning}.

When learning by imitation, the robot observe a human teacher demonstrating the skill. The fundamental difference with the teleoperation approach is the difference of embodiment between the human and the robot. This issue is referred as the correspondence problem \cite{nehaniv2002correspondence}, which the problem of mapping between the demonstrator actions (i.e. the human) and the imitator actions (i.e. the robot). For example, when demonstrating a gesture to a humanoid robot, the robot can not directly transpose the human movement to its own body as the human and the robot do not share the same body characteristic. If we consider a humanoid robot imitating the posture of a human demonstrator, the problem is better defined as reproducing the human posture as closely as possible while maintaining balance \cite{hyon2007full,yamane2009simultaneous}, where some additional constraint are provided to the robot by the system designer.

Recording the human demonstration can be done using a variety of sensors, either by adding sensors directly on the users (wearable sensors), either by using only the sensors non directly connected to the demonstrator body or relevant object. For example, using a motion capture device or using only a pair of video cameras situated in the robot's head.

Learning from imitation has been investigated in a variety of robotic application, including executing a tennis forehand swing \cite{ijspeert2002movement}, imitating arm movement \cite{billard2001learning} and hand posture \cite{chella2004posture}, object grasping \cite{lopes2005visual,tegin2009demonstration}, and also demonstration including a force component such as the fingertip force for grasping and manipulating objects \cite{lin2012learning}. Others works focused on learning by imitation a sequential task, which required to combine a sequence of multiple action to fulfill the task \cite{pardowitz2005learning,natarajan2011imitation}.

\subsubsection*{Inferring a policy}

Given a dataset of demonstration collected using one of the methods presented above, the robot should infer what action it should take in any given state to correctly fulfill the task demonstrated by the human. Learning can be as straightforward as reproducing the demonstrated behavior exactly, but most often, as the demonstration may be noisy or incomplete, the robot need to generalize from the example. We will differentiate between two approaches: \begin{inparaenum}[(a)] \item directly deriving a mapping between states and actions, i.e. a policy, from the observed data with the aim or reproducing the teacher policy, and \item the second one consist of inferring the human objective and, by using some planning method, reproducing the desired outcome without necessarily using the same actions as the demonstrator. \end{inparaenum} Roughly, the first approach is more suited for imitation while the second is more suited for emulation. 

The first approach resumes in approximating the policy function observed form the user behavior. Depending on the properties of the problem, the algorithms for learning the policy are either classification or regression techniques. 

\begin{itemize}

\item \textbf{Classification} methods are well suited for mapping discrete or continuous state to discrete actions. An example would be a robot learning to play a video game from demonstration, depending on the current state of the agent in the world, the robot should learn to press the appropriate buttons.

A large variety of classification algorithm has been used in learning form demonstration scenario. Among others, Support Vector Machines for a robot learning how to sort balls \cite{chernova2008teaching}, Hidden Markov Models have been used for an assembly task \cite{hovland1996skill}, Gaussian mixture models in a simulated driving domain \cite{chernova09jair}, but also neural networks \cite{mataric2000sensory}, beta regression \cite{montesano2009learning} or k-Nearest Neighbors \cite{saunders2006teaching}.

\item \textbf{Regression} method are well suited for for mapping discrete or continuous state to continuous actions. An example would be an autonomous car learning to steer the wheels from demonstration, given information about the surrounding environment the car should turn the driving wheel appropriately.

A large variety of classification algorithm has been used in learning form demonstration scenario, they mainly applied for learning trajectories from noisy demonstrations. Among others, Gaussian Mixture Regression for generalizing trajectories from examples in different applications \cite{calinon07}, Locally Weighted Regression for learning to produce rhythmic movement using central pattern generators \cite{schaal1998programmable,ijspeert2002learning}, Neural Networks for learning autonomous driving \cite{pomerleau1991efficient}, or Incremental Local Online Gaussian Mixture Regression for imitation learning for learning incrementally
and online new motor tasks from demonstration \cite{cederborg2010incremental}.

\end{itemize}

The second approach consists of inferring the goal of the human from demonstration. By expressing this goal as an optimization problem or as a reward function, the robot can learn to reproduce the human goal by its own means.

\textbf{Inverse reinforcement learning} \cite{ng2000algorithms,Abbeel04icml} is a popular method that is inferring the hidden reward function the demonstrator is trying to optimize based on its demonstrations. In addition, the demonstrations can be used to learn a model of the environment in state unreachable to robot by mere self-exploration. Once the reward function has been evaluated form the demonstration, and given the dynamic of the environment, the robot can generate a plan to fulfill the task using his own ability. This method is especially interesting when the human and the robot do not have the same abilities. As an example, a robot may be able to execute a skill faster that a human, be mere reproduce of the human gesture the robot would not reach the same level of performance than by inferring the underlying goal of the human.

One of the most impressive achievement of the past decade used inverse reinforcement learning methods for the learning of aerobatic helicopter flight \cite{abbeel2007application}. Demonstration were provided by an expert pilot teleoperating, i.e. flying, the helicopter to help finding its dynamics and the fitness function corresponding to different maneuvers such as flip, roll, tail-in and nose-in funnel.

\subsubsection*{Limitations and assumptions}

The performance of the learning system is obviously linked with the quality of the information provided by the demonstration. Among others aspect, if some important state-action pairs have not been demonstrated or if the demonstration were of poor quality, i.e. including a lot of noise or being suboptimal or ambiguous in certain areas, the learner will be unable to generalize properly from the data. 

Unfortunately, in many cases, the demonstration are collected beforehand and sent to the learning algorithm in a batch way which do not allow the robot to have access to better demonstration. A potential solution is to ask the teacher for new demonstrations in those state where demonstration are missing or uncertain \cite{chernova2008multi,chernova09jair}, we will details more this approaches in the next chapter.

An other problem that of identifying what the human is really demonstrating. For example, if a human is demonstrating how to fish to a robot, is the human demonstrating the precise movement of the fishing rod or demonstrating where to place the float stopper in order to catch more fish. In other words, should the robot imitates the movement of the rod fishing movement or should it emulates the position float stopper. Where imitation is the act of reproduce the human demonstration in all details, and emulation is the act of fulfill the same goal than the human demonstrated. 

This problem is currently unsolved in the robot social learning literature and in practice the robot is explicitly told whether to imitate or emulate the demonstration. The problem of understanding what to do from the interaction with human is usually solved at design time, where the system design apply a multitude of constrain the the interaction with the robot such that no uncertainty or ambiguity remains on the demonstration. For example, the demonstrated movement and provided in isolation and contains only information about the task to be learned, similarly, the robot is explicitly ``told'' that the demonstration refer to such and such object and that it is for example a grasping task. Of course saying that the robot is ``told'' about the interaction is misleading, it is rather the all system that is constrained to optimize only a specific objective.

In the context of learning from human demonstration, four central questions are often predefined at design time: who, when, what, and how to imitate \cite{nehaniv2000hummingbirds}:

The \textbf{who} question refers to the problem of identifying who to imitate, It may refer to finding which person is currently providing demonstration, but also which person is better at providing accurate demonstrations of the task. This question has not been throughly investigated in the literature so far. One of the few work tackling this problem consider a finite set of teacher and select the most appropriate one based on the robot current learning rate \cite{Nguyen2012PJBR}. This method allows the robot learner to take advantage of the different levels of skills each teacher provide.

The \textbf{when} question refers to the problems of social coordination between the two partners, such as the turn-taking ability. For example, this aspect has been investigated in human-robot drumming activities where turn-taking and role switching are important component of a successful interaction \cite{weinberg2006robot,kose2008emergent}. The when question also applies for cases where the robot should decide whether to try to imitate its human partner or to explore the environment by itself \cite{chernova09jair,Nguyen2012PJBR}. 

The \textbf{what} question refers to the problem of identifying the important aspect of the demonstrations. It refers for example to the dilemma between imitation at the action level or emulation at the effect level. At the action level, the aim of the robot would be to reproduce the demonstrator action in the same way and in the same order. At the effect level, the robot should understand the underlying purpose associated to the action of the human. 

The latter problem of identifying the effect level of imitation depends on the context in which the interaction takes place. In particular the concept of affordances \cite{gibson1986ecological} --- which encode the relation between actions, objects and, effects --- is of primordial importance for the robot to be able to reproduce demonstration at the effect level. Several works have consider affordances for human-robot learning, among others they have been used to recognize demonstrations, decompose them in a sequence of subgoals and finally reproduce them \cite{macl07affimit}. Montesano et al. presented a method to learn affordances by interacting with several objects \cite{montesano2008learning}. The robot was able to extract relation betweens its actions, the objects, and the effects it produces using Bayesian inference methods.

While most of the time the interaction protocol is well constrained such that there is no ambiguity about which aspect of the demonstration should be imitated, some social cues can be used to infer which parts of a demonstration are relevant, such as the temporal differences of demonstration parts. Pauses during interaction have been shown to be linked to important key points in a task demonstration, and allow to extract subgoals or determine when a demonstration is completed \cite{theofilis2013temporal}.

The \textbf{how} question refers to the problem of determining how the robot will actually perform the behavior so as to conform with the metric identified when answering the what question. When the demonstration is only relevant at the effect level (emulation) the robot can solve the task by its own mean as soon as the objective is identified. However when the imitation is important at the action level (imitation), differences between robot and human morphology and capabilities makes solving the how question not straightforward. This latter issue has been discussed previously and is referred to as the correspondence problem \cite{nehaniv2002correspondence}, which the problem of mapping between the demonstrator and the imitator.

\transition

As stated before, the who, when, what, and how questions are usually skipped over in practical application and the data are provided already pre-formatted for the robot.

In the next subsection we present an other paradigm for social learning in robotics, the \emph{learning from human reinforcement} approach. In this paradigm, the human never demonstrates the task to the robot but rather observe the behavior of the robot and reinforce or punish some of its actions in order to shape its final behavior. We also call this approach learning from human feedback, where feedback implies a positive or a negative assessment of the robot's actions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Learning from human reinforcement}

Learning from human reinforcement, also called shaping, is the process of learning a new skill by receiving assessment over our actions In this paradigm, the human never demonstrates the task to the robot but rather observe the behavior of the robot and reinforce or punish some of its actions in order to shape its final behavior. We also call this approach learning from human feedback, where feedback implies a positive or a negative assessment of the robot's actions. Clicker training is a subclass of this problem that consider the human can only send positive reinforcement.

Pioneers works in this domain include the work of Blumberg et al. \cite{blumberg2002integrated} which trained a virtual dog to learn several sequential tasks and associate them with
verbal cues using clicker training method. Kaplan et al. \cite{kaplan2002robotic} applied similar method to teach an AIBO robot dog. An other pionners work considered a software agent, named Cobot, that resides interact with human agents in an online chat community called LambdaMOO. Cobot adapts its behavior from various source of feedback (reward or punishment) provided by human engaged in the chat community \cite{isbell2001social}.

This social learning paradigm has share many aspects with reinforcement learning \cite{sutton1998reinforcement}. In reinforcement the agent goal is take actions so as to maximize the cumulative reward. We make a difference between reinforcement learning algorithm and learning from human reinforcement in the sense that the nature of the reward information can not be treated the same way when it is provided by a human. For example, reward signals from human are frequently ambiguous and deviates from the strict mathematical interpretation of a reward used in reinforcement learning \cite{thomaz2008teachable,Cakmak2010optimality}. We will provide more details about human teaching behaviors in the next chapter but we note that this problem requires to develop new algorithms to monitor and handle the teaching style of each user.

Therefore recent works started to investigate how to additionally learn the way the human are providing feedback at the same time as the robot learn the skill \cite{knox2009interactively}. 

However, as for learning from human demonstration, the robot should be able to infer to which actions the human feedback relates to. It may also need to differentiate between different levels of feedback as some actions may be mandatory to complete the task while others may just be preferences from the users. In addition the user could make mistakes in its assessment or may not perceive the problem as it is modeled by the robot, therefore making inconsistent feedback. And as for most learning from demonstration, most of the works presented above consider predefined and restricted interaction so as to be able to map easily the human reinforcement with the robot's actions. Similarly, if the human is providing feedback using speech commands, there exist a system that translate speech utterances into a meaningful feedback information, e.g. mapping the word ``good'' to a positive reward.

\transition

As stated above, the who, when, what, and how questions are also applicable to learning from human reinforcement problem. This question are usually skipped over in practical application and the data are provided already pre-formatted for the robot.

Providing only reinforcement signals to a robot can be limiting, especially when the state space is large which increase the learning time and results in a laboring interaction between the human and the robot. In the next subsection we present an other paradigm for social learning in robotics, the \emph{learning from human instructions} approach. In this paradigm, the human never demonstrates the task to the robot but rather observe the the behavior of the robot and provide clues, which we will call guidance, about what action to perform next.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Learning from human instructions}

Learning from human instructions, also called learning from advice, is the process of learning a new skill by receiving explicit instructions about what to do next. In this paradigm, the human never demonstrates the task to the robot but rather observes the behavior of the robot and provide clues accordingly in order to shape the robot final behavior. We also call this approach learning from human guidance, where guidance implies the user explicitly indicates to the robot what action to perform next.

In most scenario, advises are additional pieces of information allowing to improve the learning time and efficiency of an agent. It is therefore often combined with reinforcement learning algorithms where the advises influence the exploration behavior of the agent or influence directly the value of particular actions. 

In \cite{clouse1992teaching} and \cite{maclin2005giving} the teacher can influence the action selection of the agent by providing advice about preferred actions. In  \cite{smart2002effective} the trainer directly controls the agent action at important key states and let the agent learn the fine details. In \cite{kolter2007hierarchical}, the authors introduce a hierarchical apprenticeship learning method for teaching a quadruped LittleDog robot to walk on rough terrains. Their method differs from standard inverse reinforcement learning methods; rather than providing full demonstrations of the skill they consider human advices about single low level actions of the problem. More precisely, the expert indicates foot placement in situation where the robot made suboptimal foot step.

It is important for the robot to be able to generalize to previously unseen situation. In \cite{lockerd2004tutelage}, the robot Leo learns to switch all buttons on or off from human vocal instructions. When a new button is introduced in the environment the robot autonomously generalizes from the instructions and presses all buttons, instead of pressing only the one it was instructed to in the first place.

As for other learning paradigms, the robot should be able to infer to which part of the environment matters for the instructions, if the instructions can be generalized or not to other objects in the environment, if the instructions are related to what the robot should do next or what it should have done before, or in which referential are  instructions given. Ideally the robot should also keep track of other social signal received from the human, such as whether the user is really paying attention to the scene or whether the user can see the part of the space the robot is in. And as for other methods, most of the current works consider predefined and restricted interaction so as to be able to map easily the human instructions with the current important aspects of the environment and of the interaction.

\subsection{Discussion}

Our categorization of social learning paradigms  in three categories does not reflect the many subfamilies that exist between these categories, including those that are shared among categories. It is meant to situate the social learning problem in a more global picture, providing some interesting pointers for the interested readers. 

As we noticed, in most of the above presented work, the human and the robot had either no direct interaction with the robot, either few  highly constrained interactions. For example, the human demonstrations are provided in a batch perspective where data acquisition is done
before the learning phase. In the following section, we detail the usual assumptions made in most human robot interaction scenarios, and based on our observation we define the global challenge addressed in this thesis.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Usual Assumptions}

As we have seen, in most of the work in social learning there is a strong decoupling between the process of extracting useful information from the interaction and the process of learning and generalizing a new skill or task from those useful information.

On the one hand, if the goal of the robot is to learn a new task, the robot will be fed with the relevant interaction data formatted exactly in the format needed by the learning algorithm. For example, if a user should teach a robot to navigate in a maze, the protocol of interaction between the human and the robot will be ticked to match the need of the algorithm. The interaction will be turn by turn such as it is easy to associate user's instructions to robot's states. But the user will also be asked to comply to the specific signals the robot understand, such as using the word ``right'' and ``left'' to mean respectively ``right'' and ``left''.

On the other hand, when we want to learn some part of the user behavior, the task the users wants to achieved is assumed to be known. This allows to interpret the behavior of the user in the light of the known objective he is pursuing. This process is usually called a calibration phase and it is necessary to provide the robot with the ability to translate human communicative signals, such as speech or gestures, in a symbolic meaningful representation. For example, if we want our robot to learn which word the user uses to mean ``right'' and ``left'', we will ask the user to guide the robot in a maze following a specific path. The robot, knowing the path intended by the human, could identify that the human uses the word ``right'' and ``left'' or ``droite'' and ``gauche'' to mean respectively ``right'' and ``left''.

Therefore interacting with a robot present a chicken an egg problem. To teach a robot a new task, the robot must be able to understand the behavior of the human. But to come up with an understanding of the behavior of the human the robot must known what is the user overall objective. In this thesis, we present methods to overcome this chicken and egg problem in some specific cases. Before entering into more details, we should introduce a number of concepts that we will use in the following of this thesis.

\subsection{Interaction frames}

We start be defining the concept of interaction frame. An interaction frame is a structure that define all the aspects of the interaction that are pre-defined by the system designer and assumed to be followed by the human and the robot during the interaction.

The concept of interaction frame is a subclass of the more general concept of frame applied to the specific case of interaction scenarios. Frames are a concept that emerges simultaneously in social theory \cite{goffman1974frame} and artificial intelligence \cite{minsky1974framework}. They represents a stereotyped situations, a schema of interpretation given a particular situation or event. It is answering the question: \emph{what is going on here ?}, in order to reduce ambiguity of intangible topics by contextualizing the information. It creates a common ground about the purpose of the interaction \cite{tomasello2009cultural,rohlfing2013learning} and include ``predictable, recurrent interactive structures'' (\cite{ninio1996pragmatic}, p. 171). Frames thus provide interactants with guidelines about how to behave (a protocol for interaction) and also help interactants to understand the communicative intentions of their interaction partner.

The interaction frame is often implicitly assumed in robot learning experiments. We start by examplifying a few number of interaction frames and then provide a more formal description of an interaction frame.


One example of an interaction frame would be a human presenting an object to a robot at the same time as saying the name of the object. Being aware of this frame, the robot knows the utterance corresponds to the name of the object (and not its shape or its color). In this case, the label corresponding to the speech signal is given.

\paragraph{Feedback frame}
An other scenario is that of a human supervising the work of a builder robot. This builder robot is able to stack several blocks in order to form complex structures. The human wants the robot to build a specific construction but can not directly communicate the high level description of the structure to the robot. The robot only accept information about the quality of its last action. For example, if the robot took a blue cube and put it on top of a green cube, the human can assess this action and inform the robot that this particular stacking action was ``correct'' or ``incorrect'' according the human final structure wishes. After many interactions the robot is able to build the correct structure. We call this specific interaction scenario a feedback frame, where a feedback signals is defined as providing an information about the optimality of the action the robot just performed. A feedback signal can only take two values either ``correct'' or ``incorrect''.

\paragraph{Guidance frame} An other example is that of a human providing discrete action advice to a robot in the context of a reaching task. Concretely, if the human wants its robot to reach a specific room in the house, he will explicit tell the robot which direction to go in order to reach that room. For example asking the robot to go ``right'', ``left'', ``forward'', or ``backward''. The robot knows the signals of the user correspond to action it should perform to achieve the user intended goal. However the robot is not teleoperate and remains the one that select which action to perform given the advice of the teacher. For example, once the robot understood which room the user has in mind, it can go there directly without waiting for further guidance signals. We call this specific interaction scenario a guidance frame, where a guidance signals is defined as giving an information about what action to perform next.

The two latter feedback and guidance frame will be central to the future development of our work. For convenience we will refer to both feedback and guidance signals as instructions signals, in the sense that a teacher is instructing the robot using signals that may includes both feedback or a guidance instructions towards a final goal.

In our above examples, we have seen that an interaction frame regulates the interaction between the human and the robot. It includes both the constraints related to the task, e.g. teaching a robot which state to reach among a finite set of states, and the protocol used by the teacher to communicate to the robot, e.g. the teacher is assessing the robot's actions. In the context of this work, we separate the interaction frame definition in three categories:

\begin{itemize}

\item \textbf{The set of possible meanings the human can refer to.} As depicted before, a set of meaning may include ``correct'' and ``incorrect'' for those cases where the user is assessing the robot's actions. It could also be the set of action names for those case where the user provides guidances on what to do next.

\item \textbf{Details and timing of the interaction:} it corresponds to when and how the user will provide instruction signals. For example, the human sends a signal to the robot after every robot's actions. An other example is a human providing a feedback signal between 0.2 and 2 seconds after the robot's action he is referring to, like in \cite{knox2009interactively}.

\item \textbf{Constraints on the possible tasks:} The general context of the teaching process is known by the both the human and the robot. For example the robot is aware that the human wants it to reach a specific room in the house. And not to take an object in the fridge for example. This limits the number of hypothesis the robot can create about what the user has in mind.

\end{itemize}

In addition, the robot is assumed to be able to translate the communicative signals from the human into their respective meanings. For example if the human communicates to the robot using speech, then it is assumed a speech to text system is implemented and allow to convert the raw speech data into a symbolic representation.

\todo{too early}

In light of our observations, we define a generic frame function that, given a context of interaction and a task, returns the meaning intended by the teacher:
%
\begin{eqnarray}
Meaning = Frame(Context, Task)
\end{eqnarray}
%
Following our previous example, stating that if the robot moves from state 3 to state 4 (context), and that the human wants it to go in G1 (task), then the signals received from the human means ``incorrect'' (meaning), we can exemplify the use of the frame:
%
\begin{eqnarray}
``incorrect" = Frame((s3 \rightarrow s4), G1)
\end{eqnarray}





By making the interaction frames explicit, we can revise our understanding of the challenges associated to social learning. This might help us design machines more flexible to loosely defined interaction frames, or even machine that can learn the frames themselves.

A more formal description of a frame and a extended reflexion on how to leverage from more aspect of an interaction frame is presented in the thesis work of Thomas Cederborg. \todo{cite Thomas C.}


find some framework and cite thomas \cite{cederborg2013language}

\todo{make a list of many qestion that can be asked in that scope and refer to the question asked and studied in the thesis pf thomas C.}


In this work, we remove only one specific information from the frame, the ability to translate raw signals form the users to a meaningful representation for the robot. We call this kind of frame \emph{Unlabelled interaction frames} and we study how a robot can learn to cope with this lack of information. 

\subsection{Unlabelled interaction frames}

In previous section~\ref{chapter:humanexperiment:frames}, we defined the terms of interaction frames which represents a stereotyped situations, a schema of interpretation given a particular situation or event. Such frame is often assumed to be known by both the human and the robot.  which includes the signal-to-meaning classifier that translate the actual human signals into meaningful symbols for the robot. By comparing what the frame predicts and what the human actually said the robot can change its understanding of the situation.

Learning from unlabeled interaction frames correspond to the problem where the signal-to-meaning classifier is not given, and therefore the robot can not rely on a direct comparison between the prediction from the interaction frame and the observation from the human teacher.

What remains is the frame. It is at the core of our approach, it is assumed to be known by both the human and the robot. It includes both the constraints related to the task, e.g. teaching a robot which state to reach among a finite set of states, and the protocol used by the teacher to communicate to the robot, e.g. the teacher is assessing the robot's actions. Therefore, the meanings of the unlabeled signals is not explicitly given but is known to belong to a finite set of possible meanings.

We define a generic frame function that, given a context of interaction and a task, returns the meaning intended by the teacher:
%
\begin{eqnarray}
Meaning = Frame(Context, Task)
\end{eqnarray}
%
Following our previous example, stating that if the robot moves from state 3 to state 4 (context), and that the human wants it to go in G1 (task), then the signals received from the human means ``incorrect'' (meaning), we can exemplify the use of the frame:
%
\begin{eqnarray}
``incorrect" = Frame((s3 \rightarrow s4), G1)
\end{eqnarray}

\transition

The idea of unlabeled interaction frame summarizes the problem of interaction we tackle in this work. However it is a quite general concept, and in order to understand the underlying principles of our algorithm, in the following sections we will restrict our analysis to simple frames and simple worlds. We will also explicit a number of assumptions related to this work.

The first assumption is that the robot and the human are aware of the frame in which the interaction take place. This frame regulates the interaction between the two partners, it includes:

\begin{itemize}

\item \textbf{The set of possible meanings the human can refer to.} As depicted before, the set of meaning includes ``correct'' and ``incorrect'' for thoses cases where the user is assessong te robot's actions. It could also be the set of action names for those case where the user provides guidance on what to do next.

\item \textbf{Details and timing of the interaction:} it corresponds to when and how the user will provide instruction signals. For example, the human sends a signal to the robot after each action the robot performs. An other example may be the human providing a feedback signal between 0.2 and 2 seconds after the robot's action he is referring to, like in \cite{knox2009interactively}.

\item \textbf{Constraints on the possible tasks:} For example the robot is aware that the human wants to have it grasp one object on the table. And not reach for an object in a different room. This limits the number of hypothesis the robot can create.

\end{itemize}

By combining those three aspects of an interaction frame, the robot can create a set of interpretation hypothesis for the received teaching signals. For one possible task, and given a specific context (e.g. state and action performed in the environment), the robot can infer the meaning intended by the human user. By doing so for each possible task, it creates a set of interpretation hypothesis, which we rely on to fond the task taught by the user, as well as the signal to meaning mapping.


\section{Thesis Contributions}

The main contribution of the thesis is a method allowing a robot to learn from unlabelled interaction frame. In practice, it allow a user to start teaching a robot a new task using its own preferred teaching signals. For example, a user provides, using for example speech commands, instructions to a robot about what action to perform next, with our method, the user is not restricted to a pre-defined set of words and can rather use its preferred words. The system will learn which words are associated to which meaning as well as identifying the task the user wants to solve. The user could therefore use words in english, french or spanish, but also interjections or even hand clapping.

In more details, we can highlight four important contributions of this thesis:

\begin{itemize}

\item We propose a new experimental setup to study the co-construction of interaction protocols in collaborative tasks with humans \cite{vollmer2014studying} (chapter~\ref{chapter:humanexperiment}). In this setup, an architect and a builder must communicate using a restricted 10-bit channel in order to achieve the joint activity of constructing a structure using simple building blocks. We report experiments with human subjects which indicates that the kinds of meanings the participants coordinate on is limited some specific subsets such as feedback (``yes'', ``no''), guidance (``left'', ``right'', ``assemble''), feature based (``red'', ``small''), or global (``end'', ``reset'') instructions. Especially most of the user seems to concentrate on the ``yes'' and ``no'' kind of instruction. Finally we report that humans solve the problem by projecting the interaction into different common frames of
interaction, which is the basic principle used in the algorithms presented in this thesis.

\item  We present an algorithm that allow to simultaneously learn a new task from human instructions as well as the mapping between human instruction signals and their meanings \cite{grizou2013interactive,grizou2013robot,grizou2014robot,grizou2014calibration,grizou2014interactive} (chapter~\ref{chapter:lfui}). In practice, it allows a user to start teaching a new task to a robot using his preferred teaching signals and without going through the usual calibration phase.  

Our method consists of generating interpretation hypothesis of the teaching signals with respect to a set of possible task. We will see that the correct task is the one that explains better the history of interaction. We demonstrate the efficiency of our method in a pick and place scenario where a user use spoken words to instruct the robot to build a specific construction. We show that our method works if the teacher provide feedback (``correct'' or ``incorrect''), or guidance (``left'', ``right'', \ldots) instructions to our robot. Finally we show that our system can reuse the knowledge acquired about the signals of the users during the learning of a first to learn a second task faster.

\item We propose a measure of uncertainty on the joint task-signal space that takes into account both the uncertainty inherent to the task, which is unknown and remains to be estimated, as well as the uncertainty about the signal to meaning mapping, which is also unknown and remains to be estimated \cite{grizou2014calibration,grizou2014interactive} (chapter~\ref{chapter:planning}). Measuring uncertainty allows to optimize the action selection of our agent which improves significantly the learning time.

\item We apply our algorithm to brain computer interfaces (BCI) \cite{grizou2013zero,grizou2014calibration} (chapter~\ref{chapter:bci}). We present experiments where several subjects control an agent from scratch by mentally assessing the agent's actions and without requiring a calibration phase to train a decoder of the user's brain signals. In all experiments, our algorithm was able to identify a first task in less iterations that a usual calibration procedure requires.

\end{itemize}

\section{Thesis Outline}

The first aim of this manuscript is to explain the problem of learning from unlabeled interaction frames and to provide an intuition on what properties can be exploited to solve this problem. We will introduce the most important aspects of the work by simple visualization of the problem and of the specific properties we exploit. Our objective is therefore to endow the interested readers with sufficient understanding of the problem to implement their own version of the algorithm with the tools they are more familiar with.

In chapter~\ref{chapter:relatedwork}, we present an overview of the related work which span from language acquisition to brain computer interfaces.

In chapter~\ref{chapter:humanexperiment}, we introduce a new experimental setup to study the co-construction of interaction protocols in asymmetric collaborative tasks with humans. By presenting preliminary results based on this setup, we derive interesting lessons for our problem. This work on human experiment is a joint collaboration with Anna-Lisa Vollmer and Katharina J. Rohlfing.

In chapter~\ref{chapter:lfui}, we introduce in more specific terms the problem and provide a visual intuition on what properties we will exploit. We continue by formalizing the problem in a probabilistic framework, describe how each subcomponent of our algorithm are implemented and present results from a robotic pick and place scenario.

In chapter~\ref{chapter:planning}, we introduce the planning specificities related to our problem and provide a visual intuition on what properties we should track. We continue by defining the uncertainty measure used in the planning process and demonstrate on a 2D grid world problem the efficiency of the method with respect to other planning strategies.

In chapter~\ref{chapter:bci}, we present an application of the algorithm to a BCI scenario where a subject control an agent on a grid. We report online experiment showing that our algorithm allows untrained subjects to start controlling a device without any calibration procedure and by mentally assessing the device's actions. This work on BCI is a joint collaboration with I{\~n}aki Iturrate and Luis Montesano.

In chapter~\ref{chapter:limitations}, we discuss a number of limitations and propose several methods to overcome them. The limitations include the use of a discrete state space, the need for a finite set of task hypothesis and the fact that the interaction frame is uniquely defined in advance. We further propose a proof for our algorithm in restricted conditions.