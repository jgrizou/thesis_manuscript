%!TEX root = ../../thesis.tex
\define{\chapterpath}{relatedwork}
\define{\imgpath}{\chapterpath/img}

\chapter{Related Work}
\label{chapter:relatedwork}
\minitoc


In most robot social learning experiments today, there is a strong decoupling between the process of extracting useful informations from the interaction and the process of learning a new skill from these informations. For example, the human demonstrations were provided in a batch perspective where data acquisition is done before the learning phase. The properties of teaching interactions with a human in the loop was not yet considered in depth.

In this chapter we highlight the difference between systems learning from well controlled interactions and systems trying to close the interaction loop allowing more flexibility in the interaction process. These issues have began to be addressed in a subfield called \emph{interactive learning}  which combine ideas of social learning with extrinsic and intrinsic motivated learning. With this approach, the robot acquires a form of autonomy with respect to how to deal with the human in the loop. 

After presenting the related work in interactive learning, we broaden the scope of this work by linking with the computational modeling of language, some aspects of unsupervised learning, and specific works on ad-hoc team whose stated challenge is to enable cooperation without coordination in multi-agent scenarios. Finally, we present related works from the brain computer interfaces (BCI) community.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Interactive Learning}

In this section, we present a number of works considering the human component into the learning loop. We call this area of research \emph{interactive learning} \cite{nicolescu2003natural,breazeal2004tutelage}. It aims at developing machines that can learn by practical interaction with the user.

% Most of the systems presented in the previous section have not considered in depth the properties of teaching interactions with a human in the loop. The demonstrations are provided in a batch perspective where data acquisition is done before the learning phase.

\emph{Interactive learning} combines ideas of social learning with extrinsic and intrinsic motivated learning. It differs from the works presented in introduction in the sense that both the human and the robot are simultaneously involved in the learning process \cite{kaplan2002robotic,nicolescu2003natural,breazeal2004tutelage,thomaz2008teachable}. Under this approach, the teacher interacts with the robot and provides extra feedback or guidance. But in addition, the robot can act to improve its learning efficiency or elicit specific responses from the teacher. Recent developments have considered: extra reinforcement signals \cite{thomaz2008teachable}, action requests \cite{macl09airl}, disambiguation among actions \cite{chernova09jair}, preferences among states \cite{Mason2011}, iterations between practice and user feedback sessions \cite{judah2010reinforcement} and choosing actions that maximize the user feedback \cite{knox2009interactively}.

We decided to split this related work in four categories. Firstly, we present works combining multiple sources of information, such combining demonstration and feedback. Secondly, we present some studies about the behavior of human teaching robots. Thirdly, we present work that try to model some aspects of the user behavior or of the protocol. Fourthly, we present some work were the robot is active, trying to learn faster from or about the interaction. Finally, we discuss and situate our work in this scope.

% Interactive learning \cite{nicolescu2003natural,breazeal2004tutelage} aims at developing systems that can learn by practical interaction with the user and finds applications in a wide range of fields such as human-robot interaction, tutoring systems or human-machine interfaces.
% This type of learning combines ideas of learning from demonstration \cite{argall09survey}, learning by exploration \cite{thrun1992efficient} and tutor feedback \cite{kaplan2002robotic}. Under this approach the human teacher interacts with the machine and provides extra feedback or guidance. 
% In addition, the device can act to improve its learning efficiency. Approaches have considered: extra reinforcement signals \cite{thomaz2008teachable}, action requests \cite{lopes2009active}, disambiguation among actions \cite{chernova09jair}, preferences among states \cite{Mason2011}, iterations between practice and user feedback sessions \cite{judah2010reinforcement}, and choosing actions that maximize the user feedback \cite{knox2009interactively}. 

\subsection{Combining multiple learning source}

Researchers in the domain have considered mixing different types of paradigms in order to improve the quality of the interaction and of the learning process. They considered:

\begin{itemize}

\item Mixing environmental rewards with human rewards \cite{knox2010combining,griffith2013policy,grave2013learning}. The main problem is to balance the influence of the environmental reward with the human generated reward.

\item Iterations between practice and user feedback sessions \cite{judah2010reinforcement}. The learner first practices the task a few times and learns from some environmental reward. Then a user can observe it practice session and classifier some policies or actions as good or bad. The learner update its policies given the reward from the environment and the user critiques. And the process repeats again.

\item Giving some demonstrations first, and having the robot practicing the skill under human supervision (feedback or guidance) \cite{nicolescu2003natural,pardowitz2007incremental}.

\item Mixing concrete instructions and rewards to balance human efforts with communication efficiency \cite{pilarski2012between}.
% between instruction and reward to balance human effort with communication efficiency. Assistive technology, it is rare that you can control all aspect of the technology at once, for example, many active hand prosthesis have several modes of operation, two finger grip or full hand grip for example, where then fine control should be apply within that mode. Learning the preference of the user in terms of switching time based the past experience on the interaction. If what the system is not good, then the user can manually change it back. 

\item Combining learning from demonstration and mixed initiative control \cite{grollman2007dogged}. Mixed initiative control is when the control can transition smoothly from the demonstrator control to the robot control. In \cite{grollman2007dogged} the authors used this method to teach different behaviors to a robot, such as mirroring the head position with the tail position, or to seek for a red ball using the same algorithm.

\item Combining transfer learning, learning from demonstration and reinforcement learning \cite{taylor2011integrating}.

\item Considering demonstrations of parts of trajectories only. In \cite{akgun12hri}, the users only demonstrate some keyframe position along the line of the trajectory. The robot can then autonomously infer a trajectory that math with each keyframe position.

\end{itemize}

But researchers also created new learning paradigms, such as learning from users' preferences \cite{Mason2011,akrour2011preference}. In this new paradigm, the system learns the preferences of the human and will pro-actively generalize and apply them autonomously. 

In \cite{Mason2011}, the user starts by teleoperating the robot and can mark some states as good or bad. From this data, the robot can create a user profile. Next the robot can select its own goal without the need for human teleoperation. Once desirable state of the world has been reached, the human as a possibility to classify the state as good or bad again. The robot can update its user profile, and the process iterates.

% We have developed a robotic system that interacts with the user, and through repeated interactions, adapts to the user so that the system becomes semiautonomous and acts proactively. In this work we show how to design a system to meet a userâ€™s preferences, show how robot pro-activity can be learned and provide an integrated system using verbal instructions. All these behaviors are implemented in a real platform that achieves all these behaviors and is evaluated in terms of user acceptability and efficiency of interaction

In \cite{akrour2011preference,akrour2012april,akrour2014programming,wilson2012bayesian}, the robot demonstrates some candidate policies and ask the human to rank them by preferences. Based on this ranking the algorithm learns a policy scoring function, which is later used to generate new policies. These new policies are ranked again by the user, and the process iterates. This method differs from the learning from human reinforcement paradigms as the user evaluates full demonstrations. It differs from inverse reinforcement learning because the robot is it-self generating the demonstrations. But more importantly, demonstrations are ranked between them, which differs from the usual assumptions that all demonstrations given to the learning algorithm are equally correct but noisy.


% \cite{akrour2014programming} programming by feedback. The agent it doing the full demonstration and the human ranks the demonstration. With an active selection of the demonstration by the agent. Approximating user utility function, taking noise and errors into account. Looks like IRL but I don't understand how it differ. \cite{akrour2011preference} Preference-based Policy Learning, iterates a four-step process: the robot demonstrates a candidate policy; the expert ranks this policy comparatively to other ones according to her preferences; these preferences are used to learn a policy return estimate; the robot uses the policy return estimate to build new candidate policies. \cite{akrour2012april} Iteratively, the agent selects a new candidate policy and demonstrates it; the expert ranks the new demonstration comparatively to the previous best one; the expert's ranking feedback enables the agent to refine the approximate policy return, and the process is iterated. the lesson learned from the experimental validation of April is that a very limited external information might be sufficient to enable reinforcement learning: while mainstream RL requires a numerical reward to be associated to each state, while inverse reinforcement learning [1,18] requires the expert to demonstrate a sufficiently good policy, April requires a couple dozen bits of information (this trajectory improves / does not improve on the former best one) to reach state of the art results. In most problem some delays is always present between an action and its effective reward. How it compares with eligibility traces \cite{sutton1998reinforcement} in RL? which can significantly speed learning. to handle delayed reward. each time a state is visited, it initiates a short -term memory process, a trace, with then decays gradually over time. This trace marks the state as eligible for learning. Compared to Knox, the reward is at the end not during the exp, make it more difficult to identify which part is correct. \cite{wilson2012bayesian} similar but with active learning as well. use small bits of demonstration. Active request. We consider the problem of learning control policies via trajectory preference queries to an expert. In particular, the learning agent can present an expert with short runs of a pair of policies originating from the same state and the expert then indicates the preferred trajectory. The agent's goal is to elicit a latent target policy from the expert with as few queries as possible. To tackle this problem we propose a novel Bayesian model of the querying process and introduce two methods that exploit this model to actively select expert queries. Experimental results on four benchmark problems indicate that our model can effectively learn policies from trajectory preference queries and that active query selection can be substantially more efficient than random selection.

Some of the methods above consider the users are somehow optimal or at least predictable in their teaching behaviors. However this is not always the case, in next subsection we review studies about the behaviors of humans when teaching robots.

\subsection{How people teach robots}

An important challenge is to deal with non-expert humans whose teaching styles can vary considerably. Users may have various expectations and preferences when interacting with a robot and predefined protocols or instructions may bother the user and dramatically decrease the performance of the learning system \cite{thomaz2008teachable,kaochar2011towards,knox2012humans,rouanet2013impact}. These studies show that even when using well defined protocols, it is important to consider how different instructions can be used for learning.

People will not always respect predefined conventions. Several studies discuss the different behaviors naive teachers use when instructing robots \cite{thomaz2008teachable,Cakmak2010optimality}. When learning from human reinforcement, an important aspect is that the feedback is frequently ambiguous and deviates from the mathematical interpretation of a reward or a sample from a policy. For instance, in the work of A. L. Thomaz et al. \cite{thomaz2008teachable} the teachers frequently gave a positive reward for exploratory actions even if the signal was used by the learner as a standard reward. Also, even if we can define an optimal teaching sequence, humans do not necessarily behave according to those strategies \cite{Cakmak2010optimality}. This is often because the user and the robot do not share the same representation of the problem.

For the specific case of learning from human reinforcement, several works studied how people actually teach by explicit reward and punishment.
In \cite{thomaz2006reinforcement}, they found that people gave more positive than negative rewards. Also, users tend to use feedback signals to give guidance to the agent and to encourage the agent in its exploratory actions. In \cite{knox2009design}, the authors shows that humans reinforce almost always state-action pairs and not state only. People perceive intentionality in the robot's actions, and therefore human trainers reinforce when the expected long term returns of an action. They do not provide a solely immediate reward as reinforcement learning algorithm rely on. Human teachers reinforce what the robot is about to do (intentionality) or what it just done. Therefore the question of how to divide human feedback between future or past actions is not obvious yet. In addition, human reinforcement behavior is a moving target and can not be considered as an sampled from an immutable hidden reward function. Finally, in \cite{loftinlearning}, the authors studied the role of non explicit feedback. Some users do not always give explicit feedback in response to a robot's action. For example, they have shown that some users are more likely to provide positive feedback than negative feedback. But also that some people might never give positive feedback. This variety of user profile makes it difficult to create a general algorithm for learning from human reinforcement. However, if the users are consistent in their strategies, it might be possible to model and exploit them individually. 

Given these observations, considering people as optimal teaching agents seems flawed. What is optimal for a robot, in a mathematical sense, may not be experienced as optimal by every users. And more importantly it might be experienced differently by each user. There is a number of design principle that has been derived from such experiments to create better interactive learning systems.

\paragraph{Transparency} It is therefore important for the user to understand the way the robot ``thinks'' and what are its ``intentions''. A learner which display its current ``state of mind'' is called a transparent learner \cite{thomaz2008teachable}. A simple example would be a robot that displays its current level of understanding of the task using a colored LED. The robot could also directly vocalize its understanding of some part of the problem, or if it did not understand some words from the teacher \cite{chao2010transparent}. And other option is a robot demonstrating what it has understood so far while asking for confirmation or correction \cite{cakmak2012designing}.

Also it may be useful to characterize the preferences of users in terms of teaching behavior. In \cite{cakmak2012designing}, Cakmak et al. used human-human experiments to find out which types of question were most often used. Based on their observations, queries about features of the problem were identified as the most common questions. They were also perceived as the smartest when used by the robot. Using this method the robot explicit test some aspects of the task and ask to the teacher: ``can I do that?''.

\paragraph{Controlling the leader/follower balance} Asking feedback from the user is more useful when allowing to differentiate ambiguous states. In \cite{chao2010transparent} active learning is shown to improve the accuracy and efficiency of the teaching process. However active learning may illicit undesirable effects of acceptability regarding the leader-follower balance of the interaction. In \cite{chao2010transparent}, some people felt uncomfortable when the robot asked too many questions and did not feel like their were the teacher, i.e. the one leading the interaction. As a conclusion, the interaction is best accepted when a proper balance is achieve between autonomy, feedback request and human control. A robot asking a question every step is boring for the user, and asking too infrequently is unpredictable. Also allowing the user to send feedback when he wanted was preferred, but was in the end less efficient.

% derived principle: \cite{thomaz2008teachable}  transparency, balance of control leader follower \cite{cakmak2010designing} led to conclusion about balance of autonomy and control. a question every step is boring, and asking sometime is unpredictable. Letting the user send feedback when he wanted was preferred but less efficient.

\paragraph{Testing the robot} As a kind of transparency, it is important for the teacher to be able to ask the learning agent to perform the taught skill to verify and correct it. But also to understand how the agent learns and generalize from examples. For example, in \cite{kaochar2011towards} when teachers had the opportunity to test the agent's comprehension, more than half of the participants preferred testing the student systematically after a new concept of procedure was introduced. They also showed that people tend to test the agent's more during the last third of the teaching process.

\transition

To summarize, all teachers are different and most of the time they are not optimal. Even if there is a number of design principle allowing to reduce the variability of teaching behavior, it is almost impossible to design an experiment where human teaching behavior can be predictable. Therefore modeling the user seems a natural next step.

\subsection{User modeling, ambiguous protocols or signals}

% We only focus our attention on the modeling of human users by a robot and during a teaching interaction. 

Modeling the user as the interaction goes by is primordial to adapt to an a priori unknown human. Some works investigate how to learn the user teaching behavior online \cite{knox2009interactively}, how to learn the meaning of new human signals starting from a set of known signals \cite{macl11simul,loftinlearning}, or to directly learn the meaning of unknown signals but when the agent has access to a direct measure of its performance \cite{branavan2011learning,kim2012unsupervised,doshi2008spoken}.

In \cite{knox2009interactively}, an artificial agent learns from human reinforcement but the human signals are not treated as a reward in a reinforcement learning problem. Instead the the agent models the trainer reinforcement function, and considers it as a moving target. The idea is that the human reinforcement already includes the long terms consequences of the agent's action, whereas in reinforcement learning the reward act just locally. Therefore Having modeled the user reinforcement function, the agent can act greedily on this function to achieve the desired task. Their approach has been extended to continuous states and actions \cite{vien2013learning}.

In \cite{macl11simul}, the learning agent receives signals of both known and unknown meaning. The agent learns a task using the known information and is then able to infer the meaning of the a priori unknown signals. Similarly in \cite{loftinlearning} the agent learns the meaning of non-explicit signals, e.g. when the user do not press any button, but knowing the meaning of all explicit signals. Our problem differs because we do not have access to a set of signals of known meaning beforehand.

In \cite{branavan2011learning}, the learning agent automatically extract information from a text manual to improve its performance on a task. The agent learns how to play the strategy game Civilization II, and it has access to a direct measure of its performance. But the agent also have access to the game manual which give some explanation about the game's strategy. However the agent do not know how to read and interpret this manual beforehand. The agent then autonomously learns to analyses the text in the manual and to use the information contained in the manual to to improve its strategy. In other words, the agent learns the ``language'' of the game manual. While the agent could learn to play the game alone, their results show that ``a linguistically-informed game-playing agent significantly outperforms its language-unaware counterpart''. Our problem differs because our agent do not have access to a measure of its performance on the task, and can only rely on the unlabeled signals received from the teacher. However we will process much simpler signals without syntactic structure.

Some other works have focused on learning semantic parsers, either from natural language as text \cite{branavan2011learning,kim2012unsupervised} or real speech \cite{doshi2008spoken}. Semantic parsers allow for a more natural human-robot interaction where more advanced set of instructions can be used. While in \cite{kim2012unsupervised} the algorithm can produce, with some limitation, previously unseen meaning representation. However these works assume the agent has access to a known and constrained source of information about the task. Either a direct access to its performances \cite{branavan2011learning}, to a reward from a teacher \cite{doshi2008spoken}, or to a tuple (text instruction sentence, state, action sequence) where the instruction describes at a higher level the observed action sequence \cite{kim2012unsupervised}.

\transition


Modeling parts of the user behavior allows an interactive learning agent to adapt to a variety of teaching behaviors. The work presented in this thesis follows along the same lines. We learn the signal to meaning mapping of the user's signals. And contrary to the works presented above, we simultaneously estimates the desired task, and do not have access to a measure of our performance or to others known signals. It allows a user to teach a machine a new task using teaching signals unspecified in advance. As a consequence, if using speech as the modality of interaction, our system should handle different languages or even interjections or hand clapping.

\subsection{Active learners and teachers}

Finally an other crucial aspect for an efficient interaction is to have both a learner and a teacher that seek to maximize the learning gain of the learner. We usually call this types of agent active learners or active teachers. An active learner will seek for situation in which it feels uncertain about what to do, that are situation where more information is needed. An active teacher will try to provide the most useful demonstrations or instructions to the learning agent. Ideally an active teacher considers the learning capabilities of the learner to adapt its teaching behavior. 

\paragraph{Active learners} 

The interested reader can refers to \cite{lopes2014active} for a review of active learning for autonomous intelligent agent. In the following paragraphs, we only focus on active learning agent in social interactive learning conditions. The notion of uncertainty is often used in active learning agents. Uncertainty refers to situation where the agent do not know how to behave in order to fulfill the task. By collecting more information about that situation, the agent should reduce uncertainty and increase its performance on the task.

A number of previously presented work already includes an active component to their agents. For example, in \cite{macl11simul}, the agent is shown to be more efficient at learning both the task and the meaning of new signals when seeking for uncertain state-action pairs. In \cite{judah2012active}, the authors considers active imitation learning. Instead of passively collecting demonstration from the user, the learning agent query the expert about the desired action at specific states. In \cite{chernova09jair}, the authors propose to balance autonomy and demonstration request using a measure of confidence. The robot asks for demonstration only in states it is unsure about what to do. Otherwise the robot acts autonomously but can still be corrected by the user at any time.

\todo{add info from the review of manuel, + follow some links}

% \cite{lopes2012strategic} strategic student metaphor: a student has to learn a number of topics (or tasks) to maximize its mean score, and has to choose strategically how to allocate its time among the topics and/or which learning method to use for a given topic. maximize learning gain is optimal. 


\paragraph{Active teachers}

An active teachers try to provide demonstrations or instructions that will make the learning process more efficient for the learning agent. 

In \cite{cakmak2012algorithmic}, the authors study how a teacher can optimally provide demonstrations for a sequential problem. Concretely, the teacher should find the smallest sequence of examples that allow the learner to identify the task. They found that \todo{what they show + follow some links}. Similarly in \cite{torrey2013teaching}, the teacher as a limited number of advises to give and the authors study how to best use this advise credits to improve the learning gain of the learning agent. They showed that advices can have greater impact when it is spent on important states, or to correct agent's mistakes.

Active teaching finds applications in several domains, especially in the educational one, where giving individual advises for each student and given their individual proficiency may improve the collective learning gain of a classroom. For example, in \cite{clement2014online} the authors presents an \emph{intelligent tutoring systems} which ``adaptively personalizes sequences of learning activities to maximize skills acquired by each student''. They take into account constraint about the limited time and motivation resources of each student. Their approach seek at optimizing the learning gain of each student, their algorithm selects the exercise that should make the student progress best.

\transition

We will also consider an active learner, differ in that both the task and the signal to meaning mapping is unknown. We will see that this reauired to devellop a new uncertainty measur specific to our problem

\subsection{Discussion}

This thesis lies around those challenges. We identified one important assumption made in all those experiments. The facts that the human and the robot are assumed to be able to undertaker each others on one level or an other before the interaction can begin. Whether the meaning of the social signals are known and the robot should infer the task, or the task is known and the robot should infer some characteristic of its human partners, whether the mapping between some communicative signals and their meaning, or the frequency to which it tends to use some words.


A usual assumption in such systems is that the learner and the teacher share a mutual understanding of the meaning of each others' signals, and in particular the robot is usually assumed to know how to interpret teaching instructions from the human. In practice, the range of accepted instructions is limited to the one  predifined by the system developer.

 We believe that robots should themselves be able to adapt progressively to every user's particular teaching behaviors at the same time as they learn new skills

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Language Acquisition}
\label{chapter:related:language}

stated in the language acquisition but really the closest work form ours. \cite{cederborg2011imitating} probablyt he closest work to this work, there is several trajectory, and several referential. by making hypothesis that each trajectory refer to each of the referential, we can find out which on belong to which referential and find which trajectories belong together. can reproduce a gesture wrt. an other combination of movement object. 
difference: the association related to the referential, the frame is direct reasoning in the agent head, the robot do not know how to act in the first place, learnthe gesture and generalize reproduction in other coordinate systems.

Although learning from completely generic feedback types is very hard, we follow the approach from \cite{griffiths2012bottom}, which explores a human to human interaction in a categorization task where instruction can only be provided via six unlabeled symbols (thus the meaning of teaching signals are unknown to the learner). This study shows that tutors seem to spontaneously use three main types of instruction in order to help the learner: positive feedback, negative feedback, and concrete instructions (e.g. name of next optimal action). Yet, to our knowledge, nobody has investigated this problem with artificial agent in the context of human-robot interaction.

Yet, this capability is crucial in infant social development and learning, as well as in adult mutual adaptation of social cues. This has been the subject of experiments in experimental semiotics \cite{galantucci2009experimental}, such as in the work of Griffiths et al. \cite{griffiths2012bottom} who conducted an experiment with human learners learning the meaning of unknown symbolic teaching signals. The experimental setup we present here in the context of human-robot interaction is a variant of theirs, where teaching signals are sub-symbolic and not from a pre-determined set.



While this is not the main target of this article, this work is also relevant with regards to the computational modeling of language acquisition. The general question of how certain sub-symbolic communication signals can be associated to their meanings through interaction has been largely studied in the literature so far. But the specific question of how teaching signals (speech words here) can be mapped to teaching meanings and how they can be used for learning new tasks has, to our knowledge, not been modeled computationally so far. 

As argued in the previous section, the work we present has also some relevance with regards to the computational modeling of language acquisition. The literature on computational modeling of language acquisition by machines and robots is large and diverse, and focused on many aspects of language learning \cite{steels2012grounding,steels2002aibos, cangelosi2010integration, kaplan2008computational, steels2003evolving, brent1997computational, yu2007unified}. An important line of work investigated the Gavagai problem \cite{quine1964word}, i.e. the problem of how to guess the meaning of a new word when many hypothesis can be formed (out of a pointing gesture for example) and it is not possible to read the mind of the language teacher. Various approaches were used, such as constructivist and discriminative approaches based on social alignment \cite{steels06spatialLanguage, steels2008can}, pure statistical approaches through cross-situational learning \cite{xu2007word, smith2008infants} or more constrained statistical approaches \cite{roy2005semiotic, yu2007unified}. In all these existing models, meanings were expressed in terms of perceptual categories (e.g. in terms of shape, color, position, etc) \cite{steels06spatialLanguage, steels2008can,yu2007unified}, or in terms of motor actions \cite{steels2008robot, Massera2010,sugita05a}. This applies to models implemented in robots, such as in \cite{heckmann2009teaching}, where  the robot ASIMO is taught to associate new spoken signals to visual object properties, both in noisy conditions and without the need for bootstrapping. 

Only very few models so far have explored how other categories of word meanings could be learned. \cite{cederborg2011imitating} presented a model where word meaning expressed the cognitive operation of attentional focus, and some models of grammar acquisition dealt with the acquisition of grammatical markers which meaning operates on the disambiguation of other words in a sentence \cite{steels2012fluid}. 
The work we present in this article shows mechanisms allowing a learner to acquire word meaning associated to teaching and guidance in the context of social interaction. Furthermore, while in most models of lexicon  acquisition no tasks are learned (only word-meaning associations are learned), we show mechanisms allowing the learner to leverage learned word meanings to learn novel tasks from a human.

Finally, the work of Steels and colleagues \cite{steels2012grounding,steels2002aibos} have extensively shown the importance language games, instantiating various families of pre-programmed interaction protocols specifically designed to allow robots to learn speech sounds\cite{de2000self,oudeyer2006self}, lexicons \cite{steels2002aibos} or grammatical structures \cite{steels06spatialLanguage, steels2008can}. Other works used similar interactional frames to allow a structured interaction between humans and robots so that new elements of language could be identified and learnt by the robot learner \cite{roy02a,lyon2012interactive,cangelosi06b,yu2004multimodal,cangelosi2010integration,sugita05a,dominey2005learning,cederborg2011imitating}. In particular, it was shown that these interaction protocols fostered efficient language learning by implementing joint attention and joint intentional understanding between the robot and the human \cite{kaplan2006challenges,yu2005role,yu2007unified}, for example leveraging the synchronies and contingencies between the speech and the action flow \cite{rohlfing2006can,schillingmann2011acoustic}. Yet, to our knowledge, in all these existing works the interaction protocols have always been pre-programmed (that is the robot knows how to use and understand them innately, e.g. he knows how the teacher expresses ``good'' or ``bad'' feedback), as well as used by the robot learner to strictly identify new (form,meaning) pairs in the sensorimotor flow. The methods we present here form a basis on which such flexible and adaptive teaching protocols could be learnt.

\cite{spranger2013evolutionary} landmarks have an important function in spactial language. it can represent aocmmon ground, easier to refer to for each partners involved in the language game. it allow to evolve spatial relation. better with landmark than without. effect on success in communication and their impact on the
formation of spatial relations.
\cite{spranger2012emergent} \cite{spranger2013grounded} grounded acquisition experiments of increasing complexity. how various spatial language systems, such as projective, absolute and proximal can be learned. The proposed learning mechanisms do not rely on direct meaning transfer or direct access to world models of interlocutors.
\cite{spranger2012co} This chapter studies how basic spatial categories such as left-right, front-back, far-near or north-south can emerge in a population of robotic agents in co-evolution with terms that express these categories. It introduces various language strategies and tests them first in reconstructions of German spatial terms, then in acquisition experiments to demonstrate the adequacy of the strategy for learning these terms, and finally in language formation experiments showing how a spatial vocabulary and the concepts expressed by it can emerge in a population of embodied agents from scratch. Showed that the principles of selection and self-organization which were successfully used in earlier chapters to study the emergence of proper names, color terms and body postures could also be applied to study the emergence of basic spatial term

lingodroid \cite{schulz2010robots} : use iRats, have shared attention, when they encounter one antoher they communicate: where are we, what time of the day is it (use a wird and measure the light level), and meet-at a location at a specific time. The concept of morning, afternoon are changing with the season according to the lightning cycle. \cite{schulz2011lingodroids}
\cite{heath2012long} cocevolve geopersonal spacial language and language for time event, while building the map (no language a priori defined). base donly on the light level (simulated no noise) for example, day-night cycle. one year long period. gorunding words, concep of cyclic time throug experience wth events rather than by time or calendar.


difference with language games and our work: the realtion is direct, it applies to where we are now, the time is it now

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Multi-agent interaction without pre-coordination}

Ad-hoc teams

This workshop focuses on models and algorithms for multiagent interaction without prior coordination (MIPC). Interaction between agents is the defining attribute of multiagent systems, encompassing problems of planning in a decentralized setting, learning other agent models, composing teams with high task performance, and selected resource-bounded communication and coordination. There is significant variety in methodologies used to solve such problems, including symbolic reasoning about negotiation and argumentation, distributed optimization methods, machine learning methods such as multiagent reinforcement learning, etc. The majority of these well studied methods depends on some form of prior coordination. Often, the coordination is at the level of problem definition. For example, learning algorithms may assume that all agents share a common learning method or prior beliefs, distributed optimization methods may assume specific structural constraints regarding the partition of state space or cost/rewards, and symbolic methods often make strong assumptions regarding norms and protocols. In realistic problems, these assumptions are easily violated calling for new models and algorithms that specifically address the case of ad hoc interactions. Similar issues are also becoming increasingly more pertinent in human-machine interactions, where there is a need for intelligent adaptive behaviour and assumptions regarding prior knowledge and communication are problematic.

Effective MIPC is most likely to be achieved as we bring together work from many different areas, including work on intelligent agents, machine learning, game theory, and operations research. For instance, game theorists have considered what happens to equilibria when common knowledge assumptions must be violated, agent designers are faced with mixed teams of humans and agents in open environments and developing variations on planning methods in response to this, etc. The goal of this workshop is to bring together these diverse viewpoints in an attempt to consolidate the common ground and identify new lines of attack.

As robots are moving into the real world, they will increasingly need to group together for cooperative activities with previously unknown teammates. In such ad hoc team settings, team strategies cannot be developed a priori. Rather, each robot must be prepared to cooperate with many types of teammates, which may not share the same capabilities or communicative means. 

This challenge of collaboration without pre-coordination has been first identified in the LARG group with the work on Ad Hoc Autonomous Agent Teams \cite{stone2010ad} where agents should learn to collaborate without defining pre-coordination scheme or knowing what the other agent will be capable of. Samuel Barrett, a last year PhD student, has been the main investigator of this line of research in the LARG group. He notably investigated complex teamwork domain such as the pursuit domain \cite{barrett2011empirical}, which is the domain we will use for our common project. The first development of the Ad Hoc Team project did not consider direct communication between agent \cite{stone2010ad} \cite{barrett2011adhoc} and the collaboration were effective only through the understanding of the behavior of the other agent \cite{barrett2011empirical} \cite{barrett2013team}. More recently, Samuel Barrett et al. introduced a minimal domain with communication and proved that ad hoc team agents can successfully cooperate with unknown teammates \cite{barrett2013communicating}. This latter work considers an abstract task using an armed bandit setting. Our joint collaboration aims at extending this work to the pursuit domain. Especially we would like to investigate a domain where communication between agents is mandatory to succeed in the task. 

And in the LARG Group, the focus was more on multi-agent scenarios where robot must be prepared to cooperate with many types of teammates, which may not share the same capabilities or communicative means. The first phase of the Ad Hoc Team project considered indirect communication \cite{stone2010ad} \cite{barrett2011empirical} \cite{barrett2011adhoc} \cite{barrett2013team} and direct communication has been explored recently in an armed bandit scenario \cite{barrett2013communicating}. 

cite work of IAJ stone



 stefano work on proof.. \cite{albrecht2014uai}
On Convergence and Optimality of Best-Response Learning with Policy Types in Multiagent Systems
One method to make this problem feasible is to assume that the other agents draw their latent policy (or type) from a specific set, and that a domain expert could provide a specification of this set, albeit only a partially correct one. We analyse convergence properties of two existing posterior formulations and propose a new posterior which can learn correlated distributions. Secondly, since the types are provided by an expert, they may be inaccurate in the sense that they do not predict the agents' observed actions. We provide a novel characterization of optimality which allows experts to use efficient model checking algorithms to verify optimality of types






Recent works that address ad hoc coordination include (\cite{bowling2005coordination} \cite{gil2006dynamically} \cite{stone2010ad}). However, the assumptions made by the solutions pro- posed therein imply that they only address certain aspects of the larger problem. For example, in  \cite{bowling2005coordination} \cite{gil2006dynamically} it is assumed that all agents follow complex prespecified plans which define roles and synchronised action sequences for each role, and in (\cite{stone2010teach} \cite{stone2013teaching}) it is assumed that the other agents' behaviors are a priori known and fixed (i.e. they do not learn), and that all agents, including the ad hoc agent, have common payoffs. Furthermore, the problem descriptions in these works are of a procedural nature, associated with the specific tasks considered therein. Thus, there is a need for a formal model of the ad hoc coordination problem.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Unsupervised learning}

Unsupervised learning is the problem of finding hidden structure in unlabeled data. It mostly applies in clustering task where a dataset is divided into subgroups of data sharing similar characteristics, such as a close proximity in the feature space. 

% Even if there is no explicit error or reward to evaluate a potential solution, there is still predefined metrics which define what the terms \emph{structure} means. Finding an hidden structure is more looking from known patterns in unlabeled data.

\paragraph{Unsupervised multimodal learning} For example, in unsupervised multimodal learning, the system has access to synchronized raw information from multiple modalities. A particular instance of multimodal learning is the learning of language where the learner has to relate perception of an object to the sound of its name, or of a sound to a gesture such as in \cite{mangin2013learning}. The learner receives continuously a visual and an audio stream and should learn to associate parts of the visual information with their associated audio stimulus. But the visual and audio informations are already synchronized such that the relevant information from the visual stream is perceived simultaneously with its associated audio stimuli.

In a robotic application, Yasser Mohammad et al. used multimodal learning to segment and associated gesture commands from a user to actions of a robot \cite{mohammad2009unsupervised}. The gestures and actions were observed from a continuous stream extracted from a Wizard of Oz experiment (where the robot is secretly controlled by a human). They relied on a motif discovery algorithm to identify recurrent and co-occurrent patterns in the gesture and action flow \cite{mohammad2009constrained}. However, while being unsupervised, the stream of data where synchronized and collected using a Wizard of Oz setup, meaning that the association between the gestures and the robot's actions was provided. In \cite{mohammad2010learning} the same authors extended their approach to allow the system to actually derive controllers for the robot and not just finding recurrent patterns, as well as a methods to accumulate the knowledge learned for different partners.

\paragraph{Simultaneous localization and mapping}

Simultaneous localization and mapping (SLAM) \cite{smith1990estimating,dissanayake2001solution} is  the problem of constructing a map of an unknown environment while simultaneously keeping track of the robot's location in that environment. 

SLAM seems to include a chicken and egg problem. To build the map, the robot needs to know its location on the map such as to be able to include its current measurements to the map. And to know its location on the map, the robot needs to know the map such as to infer its position from its measurement. In practice, the answers to the two questions cannot be delivered independently of each other.

However the robot knowns that the data received from its sensors refers, for example, to noisy information about distances to obstacles. The robot also often knows the qualities of its sensors and motors, and roughly how it's actions influence its position. For example, by measuring changes in wheels rotary encoders, the robot can approximate its position shift after small control commands. Accessing to an approximation on its position shift, the robot can now update the map given its new sensory information. Using only this source of information is limiting, especially because every error accumulates over time. There is several others sources of information the robot can rely on. For example, the environment is often assume to be fixed. Hence the robot can track its relative position to some landmarks, and incrementally update the robot position in the map while detecting some other landmarks and incrementally building the map.

\transition

Unsupervised learning also deals with unknown data. But contrary to our problem, unsupervised learning only identifies direct relation between observation. To learn from unlabeled interaction frames the system must also identify a task, unknown at start, from the incoming unlabeled data. This adds another layer to our problem.

\section{Brain computer interfaces}

Interestingly, non-invasive brain-computer interfaces (BCIs) have also looked at this problem. EEG-based BCIs have been used successfully to control different devices, such as robotic arms and simulated agents, using self-generated (e.g. motor imagery) and event-related potentials signals (see \cite{chavarriaga2014errare} and \cite{millan10} for a review). 
%
Error-related potential is one event-related potential that appears when the user's expectation diverges from the actual outcome. They have been recently used as reward to teach devices a policy that solves a user's intended task \cite{chavarriaga2010learning,iturrate2010robot}.

As in most BCI applications, it is necessary to perform a calibration phase to learn a decoder (e.g. a classifier) that detects this error-related potential in a single trial. This calibration is required mainly due to various characteristics of the EEG signals: the non-stationary nature \cite{vidaurre11}; the large intra- and inter-subject variability \cite{Polich1997}, and the variations induced by the task \cite{iturrate2013task}. This phase hinders the deployments of BCI applications out of the lab and calibration free methods have been identified as an important step to apply this technology in real applications \cite{millan10}. 

There are few BCI applications that are able to calibrate themselves during operation.  For long term operation using motor rhythms, it is possible to adapt the decoder online \cite{vidaurre2010towards}. For P300 spellers, Kindermans et al. proposed a method to autocalibrate the P300 detector by exploiting multiple stimulations and prior information \cite{Kindermans2012a,Kindermans2012b}. However, Kindermans et al, due to the specific interface used (P300Speller) can guarantee that 1 signal out of 6 has a specific meaning. In our work, and in a majority of sequential problems, it is impossible to define a sequence of actions that guarantee a specific ratio of meanings in the received signals.

\cite{kindermans2014integrating} they merge most of the method they have been developing. Dynamic stopping, where the system stop when it reaches a confidence threshold. Language model as a prior probability on next letter. Transfer learning where a model of previous subjects is used to ``regularizes the subject-specific solution towards the general model.''. Unsupervised learning (that is the EM approach we refer to when citing their work)

\cite{schettini2014self} they call themselves self-calibration but it isn't in the sense we are using it. It is actually continuous adaptation of the classifier model. They have a first session using a calibration procedure, and on latter run, the system re-calibrate itself continuously.

\section{Discussion}

Still most the work, while releasing some important aspect of the interaction have access to a direct relation between signals and meaning at some point. To our knowledge, only two works are tackling the same porblem as the one presented in this thesis.

 
\cite{cederborg2011imitating}
it is not about a sequetial task. the robot can reporduce a trajectory, do not need planning skills

and

\cite{Kindermans2012a,Kindermans2012b,kindermans2014integrating} not sequetial, use an additonal source of information (specific ratio of each label). Plus they have this eureaka moment, which is the system makes some mistake in the beginning but is abla to recover them after some time. This is manily due to the nature of the task, where to spell on letter only 15 flash for each letter is allowed....

In the following of this thesis we present methods that imporve over the following work, by

sequential task

work without reauiring a specific ratio of signals

based on the coherence between the organization fo signal in the feature space and the labels. explain again the method in few lines

planning

extendtion in continuous states space, continuous task space (finite number of letter, finite number of object), release the need to known the interaction frame, i.e. feedback or guidance, unknown in advance.

proof

However we note that in the core work of this thesis, we will consider optimal teachers and simply model some percentage of teaching mistakes to account for the variability between users.


\todo{resue all this in the conlsusion}