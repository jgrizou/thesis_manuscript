%!TEX root = ../../thesis.tex
\define{\chapterpath}{relatedwork}
\define{\imgpath}{\chapterpath/img}

\chapter{Related Work}
\label{chapter:relatedwork}
\minitoc

In this section we detail the challenges

interactively teaching robots to do what we want them to do. 

We first describe the main social learning paradigms, which we organized in three main groups: learning from human demonstrations, learning from human reinforcement, and learning from human instructions. From this, we will details more the work that take the human in the loop at learning time, we call this subfield interactive learning which combine ideas of social learning with extrinsic and intrinsic motivated learning. Finally we broaden the scope of this work and links it with the computational modeling of language, some aspect of multimodal learning learning, and the work on ad-hoc team whose state challenge is to enable cooperation without coordination between robotic agents. Finally, we present related work that as emerged in the BCI community.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Robot learning from social interaction with humans}
\label{sec:intro:social}

It is often easier to acquire a new skill if someone that already acquired that skill teach us how to do it. In robotics, we investigate how knowledge can be transmitted from humans to robots through social interaction. This way human users can interact with machines without knowing how to program or how to define a fitness function for a particular task. They rather use speech, gesture and provide demonstration of the skills which has the advantage of being an intuitive way for humans to instruct robots. 

In the following of this section we present three main social learning paradigms: \begin{inparaenum}[(a)] \item learning from human demonstration, where the robot learns by imitating the human actions, \item learning from human reinforcement, where the robot learn from assessments on its own actions provided by the user, and \item learning from human instructions, where the robot learn from concrete instruction about what do to next provided by the user. \end{inparaenum}

\subsection{Learning from human demonstrations}

Learning from human demonstrations, also called programming by demonstration or learning by imitation, is the process of learning from practical examples of how to perform a specific skill \cite{schaal1999imitation,argall09survey,lopes10imitationchapter}. Demonstrating a task to a robot is more intuitive for the user than directly programming a behavior by writing lines of code. But creating a robot that learn from example requires to solves many problem with respect to the algorithm capable of inferring the task from the provided demonstrations but also to the way the robot can select and understand what part of the environment and of the interaction are relevant to the demonstrated task.

Four central questions were defined as who, when, what, and how to imitate \cite{nehaniv2000hummingbirds}:
\begin{itemize}

\item \textbf{Who} to imitate refers for example to the problem of finding which person in a group in more suited to provide correct demonstration or to identify that a person is currently providing demonstration. This question has not been throughly investigated in the literature so far. One of the few work tackling this problem consider a finite set of teacher and select the most appropriate one using intrinsic motivation processes based on the learning progress metric \cite{Nguyen2012PJBR}, which allow the robot learner to take advantage of the different levels of skills each teacher provide.

\item \textbf{When} to imitate refers for example to problems of social coordination between two partners, such as the turn-taking ability. For example, this aspect has been investigated in human-robot drumming activities where turn-taking and role switching are important component of a successful interaction \cite{weinberg2006robot,kose2008emergent}. The when question also applies for cases where the robot should decide whether to try to imitate its human partner or to explore the environment by itself. The work presented in previous paragraph \cite{Nguyen2012PJBR} makes also use of curiosity to select whether to self-explore, mimic or emulate using a curiosity based selection method.

\item \textbf{What} to imitate refers to the dilemma between imitation at the action level or emulation at the effect level. It is the problem of identifying the important aspect of the demonstrations. At the action level, the aim of the robot would be to reproduce the demonstrator action in the same way and in the same order. At the effect level, the robot should understand the underlying purpose associated to the action of the human. Such as when fixing a shelf on a wall using nails, the important aspect could be to have the shelf fixed but nails could be replaced by screw, or when poring water into a glass, to specific position of the arm may or may not be an important aspect of the demonstration, e.g. in hostelry it would matter while at home it may not be relevant. 

The latter problem of identifying the effect level of imitation depends on the context in which the interaction takes place. In particular the concept of affordances \cite{gibson1986ecological} --- which encode the relation between actions, objects and, effects --- is of primordial importance for the robot to be able to reproduce demonstration at the effect level. Several works have consider affordances for human-robot learning, among others they have been used to recognize demonstrations, decompose them in a sequence of subgoals and finally reproduce them \cite{macl07affimit}. Affordances have also been learn, in \cite{montesano2008learning}, Montesano et al. present a robot that, by interacting with several objects, is able to extract relation betweens its actions, the objects, and the effects it produces using Bayesian inference methods. 

Other sources of information have been used to infer which parts of a demonstration are more relevant, such as the temporal differences of demonstration parts. Pauses during interaction have been shown to be linked to important key points in a task demonstration, and allow to extract subgoals or determine when a task is completed \cite{theofilis2013temporal}.

\item \textbf{How} to imitate refers to the problem of determining how the robot will actually perform the behavior so as to conform with the metric identified when answering the what to imitate question. When the demonstration is only relevant at the effect level, the robot can solve the task by its own mean as soon as the end result, e.g. the end configuration of objects, is correct. However when the imitation is important at the action level, differences between robot and human morphology and capabilities makes solving the how question not straightforward.

This latter issue is referred as the correspondence problem \cite{nehaniv2002correspondence}, which the problem of mapping between the demonstrator and the imitator. This problem is obvious between a human and a robot which do not share the same body characteristic, but it also apply between two human being that do not share the exact same morphology (size, strength) but also the same sensing abilities as our individual sensory experience are certainly not experienced the same way.

At the motor level, some work have addressed this question by for example having a robot imitating the body posture of a human demonstrator. To do so the robot can not reproduce the angle between each articulation as the resulting posture is likely to be different but more importantly the robot is likely to lose its balance and fall. The problem is therefore for the robot to copy the human posture as closely as possible while maintaining its balance \cite{hyon2007full,yamane2009simultaneous}. 

At the sensory level, a typical example is for two agent to agree on the name of specific colors. Considering two robots with different cameras, how can they communicate and agree on they respective perception. Several theoretical work have addressed this issue \cite{cangelosi2001adaptive,steels2005coordinating} that are closely related to language acquisition problems which we will discuss in chapter \ref{sec:related:language}.

\end{itemize}

Those four questions remains widely open challenges and are solved in practice by constraining the interaction between the human and the robot and by defining in advance what is relevant in the demonstration. In many cases the demonstration are collected beforehand and sent to the learning algorithm in a batch way.

Among others, algorithms for learning by demonstration include regression methods based on mixture models that allow to generalize trajectories from examples \cite{calinon07}. Inverse reinforcement learning \cite{Abbeel04icml} is an other popular method that is looking for the hidden reward function the demonstrator is trying to optimize based on the provided demonstration.

One of the most impressive achievement of the past decade used inverse reinforcement learning methods for the learning of aerobatic helicopter flight. Demonstration were provided by an expert pilot teleoperating, i.e. flying, the helicopter to help finding its dynamics and the fitness function corresponding to different maneuvers such as flip, roll, tail-in and nose-in funnel \cite{abbeel2007application}.

\subsection{Learning from human reinforcement}

An other way to help a robot improves its performance on a specific skill is to provide some feedback on its actions. This social learning paradigm has share many aspects with reinforcement learning problems, however the interactive component create more ambiguous situation. 

As for learning from demonstration, the robot should be able to infer to which part of its past actions the human feedback relates to, but may also need to differentiate between different levels of feedback as some actions may be mandatory to complete the task while others may just be preferences from the users. In addition the user could make mistakes in its assessment or may not perceive the problem as it is modeled by the robot, therefore making inconsistent feedback. As an example, it has been shown that feedback signals from human is frequently ambiguous and deviates from the strict mathematical interpretation of a reward used in reinforcement learning \cite{thomaz2008teachable,Cakmak2010optimality}.

Therefore, while reinforcement learning algorithm as been initial used for robot learning from human generated rewards \cite{thomaz2008teachable}, more recent works started to investigate how to additionally learn the way the human are providing feedback \cite{knox2009interactively}. I would actually argue that learning from human reinforcement is more related to inverse reinforcement learning than to reinforcement learning. Indeed, the goal of the robot should better be to find the hidden reward function that explains the assessment of the human teacher than to maximize the rewards, or positive assessments, given by the user. We will enters those issues into more details in the next chapter.

As for learning from demonstration, most of the current works consider predefined and restricted interaction so as to be able to map easily the human reinforcement with the robot's actions.

\subsection{Learning from human instructions}

Providing feedback to the robot becomes restrictive as the space of possibles grows. If the robot has to try out all possible actions before finding the ones that elicit a positive feedback, the interaction is doomed to fail; especially for sequential problems such as chess or video games. For such cases it would be better if the human teacher could provide instruction directly to the robot \cite{breazeal2004tutelage}.

Due to the high-level nature of concrete instruction, learning from instruction is often applied with robots that are already equipped with planning skills. But were a fitness function is hard to define, or when the search space is too big to be explored efficiently. For example, in \cite{lockerd2004tutelage}, the robot Leo already knowns how to press a button before the interaction starts. The robot learns the task of swithing buttons on from human instructions, and when a new button is introduced the robot autonomously generalizes from the instruction and presses all buttons, instead of pressing only the one it was instructed to in the first place. 

As for other method, the robot should be able to infer to which part of the environement matters for the instructions, if the instructions can be generalized or not to other objects in the environement, if the instructions are related to what the robot should do next or what it should have done before, in which referential are direction instruction given, or even if the user is really paying attention to the scene. And as for other methods, most of the current works consider predefined and restricted interaction so as to be able to map easily the human instruction with the current important aspect of the environment and of the interaction.

As for learning from demonstration or learning from human reinforcement, learning from instructions could also be considered as an inverse reinforcement learning problem. One way is to consider instructions as demonstration and use learning from demonstration technics , another way is to formulate the problem such that the robot's goal is to find the hidden reward function that explains the instruction of the human teacher.

\transition

The categorization of learning paradigms above in three categories does not reflect the many subfamilies that exist between these categories, including those that are shared among categories. It is meant to situate the social learning problem in a more global picture, providing some interesting pointers for the interested readers. 

As we noticed, in most of the above presented work, the human and the robot had no direct interaction with the robot, or few well controlled interactions. For example, the human demonstrations are provided in a batch perspective where data acquisition is done
before the learning phase. The properties of teaching interactions with a human in the loop was not yet considered in depth, and this issues have began to be addressed in a subfield called \emph{interactive learning}  which combine ideas of social learning with extrinsic and intrinsic motivated learning. Where the robot acquires a form of autonomy with respect to how to deal with the human in the loop. This separation between this section and interactive learning is a bit arbitrary, we mainly want to highlight the fundamental difference between system that learn from pre-recordeed, open-loop, and  well controlled interactions from those which tries to close the interaction loop and allow more flexibility in the interaction process..

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Interactive Learning}

\cite{macl11simul}, which presented a preliminary approach to this problem considering an abstract symbolic space of teaching signals in simulation and requiring to bootstrap the system with known symbols. 

Some other works have focused on learning semantic parsers, either from natural language as text \cite{branavan2011learning,kim2012unsupervised} or real speech \cite{doshi2008spoken}. Semantic parsers allow for a more natural human-robot interaction where more advanced set of instructions can be used. While in \cite{kim2012unsupervised} the algorithm can produce, with some limitation, previously unseen meaning representation, those works assume the agent has access to a known and constrained source of information at one stage. Either a direct access to its performances \cite{branavan2011learning}; to a reward from a teacher \cite{doshi2008spoken} or to a tuple (text instruction sentence, state, action sequence) where the instruction describes at a higher level the observed action sequence \cite{kim2012unsupervised}.


\cite{mohammad2009unsupervised} learning to segment gesture command of the users and actions from a continuous stream of data , as well as they association. The gestures and actions are continuous and the authors rely on a motif discovery algorithm identify recurrent  and cooccurrent patterns in the gesture and action flows of data \cite{mohammad2009constrained}. However, while being unsupervised, the stream of data where synchronized and collected using a wizard of Oz setup, meaning that the association between the gesture and the action was provided. In \cite{mohammad2010learning} the same authors extended their approach to allow the system to actually derive controllers for the robot and not just finding recurrent patterns, as well as a method to accumulate the knowledge learned for different partners.


\cite{grollman2007dogged} combined learning form demonstration and mixed initiative control to enable life long learning for unknown task. mixed initiative control is when the control can transition smoothly form the robot control to the demonstrator control. allow to modify the task online while learning. Used successfully to teach different behavior to a robot, such as mirroring the head position with the tail position, or to seek for a red ball using the same algorithm.

\cite{pilarski2012between} between instruction and reward to balance human effort with communication efficiency.

assistive technology, it is rare that you can control all aspect of the technology at once, for example, many active hand prosthesis have several modes of operation, two finger grip or full hand grip for example, where then fine control should be apply within that mode. Learning the preference of the user in terms of switching time based the past experience on the interaction. If what the system is not good, then the user can manually change it back. Close to manuel work on preference \cite{Mason2011}. Quite closed to confidence base autonomy isn't it? \cite{chernova09jair} yes, but the agent in pilarski do not actively seek for situation to be disambiguated but as less restrained signal and task space.

\cite{akrour2014programming} programming by feedback. The agent it doing the full demonstration and the human ranks the demonstration. With an active selection of the demonstration by the agent. Approximating user utility function, taking noise and errors into account. Looks like IRL but I don't understand how it differ.
\cite{akrour2011preference} Preference-based Policy Learning, iterates a four-step process: the robot demonstrates a candidate policy; the expert ranks this policy comparatively to other ones according to her preferences; these preferences are used to learn a policy return estimate; the robot uses the policy return estimate to build new candidate policies.
\cite{akrour2012april} Iteratively, the agent selects a new candidate policy and demonstrates it; the expert ranks the new demonstration comparatively to the previous best one; the expert's ranking feedback enables the agent to refine the approximate policy return, and the process is iterated. the lesson learned from the experimental validation of April is that a very
limited external information might be sufficient to enable reinforcement learning: while mainstream RL requires a numerical reward to be associated to each state, while inverse reinforcement learning [1,18] requires the expert to demonstrate a sufficiently good policy, April requires a couple dozen bits of information (this trajectory improves / does not improve on the former best one) to reach state of the art results. In most problem some delays is always present between an action and its effective reward. How it compares with eligibility traces \cite{sutton1998reinforcement} in RL? which can significantly speed learning. to handle delayed reward. each time a state is visited, it initiates a short -term memory process, a trace, with then decays gradually over time. This trace marks the state as eligible for learning. Compared to Knox, the reward is at the end not during the exp, make it more difficult to identify which part is correct. \cite{wilson2012bayesian} similar but with active learning as well. use small bits of demonstration. Active request.


\cite{knox2009interactively} the idea is that the human as a bird eyes view on the system and the reward is not immediate but include the long terms consequences of the agent's actions. and this indication is given in a short time windows after the correct or erroneous action. The problem of credit assignment disappear.
Two steps, modeling the trainer reinforcement function H is a supervised learning problem, where for each state-action pair a value should be associated given a set of value-state-action triple. This function however is a moving target. Then act greedy on that H function.
\textbf{The shaping problem:} how to shape the policy of an agent to match the one the user as in mind. 
TAMER has been extended to continuous state and action by Vien et al. \cite{vien2013learning}.


\cite{torrey2013teaching} Teaching on a budget
a teacher agent instructs a student agent by suggesting actions the student should take as it learns. However, the teacher may only give such advice a limited number of times.  Advice can have greater impact when it is spent on more
important states. Advice can have greater impact when it is spent on mistakes. When teachers can successfully predict student mistakes, they
can spend their advice budget more effectively.
\cite{taylor2011integrating} Human-Agent Transfer algorithm that combines transfer learning, learning from demonstration and reinforcement learning




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Language Acquisition}
\label{chapter:related:language}

While this is not the main target of this article, this work is also relevant with regards to the computational modeling of language acquisition. The general question of how certain sub-symbolic communication signals can be associated to their meanings through interaction has been largely studied in the literature so far. But the specific question of how teaching signals (speech words here) can be mapped to teaching meanings and how they can be used for learning new tasks has, to our knowledge, not been modeled computationally so far. 

As argued in the previous section, the work we present has also some relevance with regards to the computational modeling of language acquisition. The literature on computational modeling of language acquisition by machines and robots is large and diverse, and focused on many aspects of language learning \cite{steels2012grounding,steels2002aibos, cangelosi2010integration, kaplan2008computational, steels2003evolving, brent1997computational, yu2007unified}. An important line of work investigated the Gavagai problem \cite{quine1964word}, i.e. the problem of how to guess the meaning of a new word when many hypothesis can be formed (out of a pointing gesture for example) and it is not possible to read the mind of the language teacher. Various approaches were used, such as constructivist and discriminative approaches based on social alignment \cite{steels06spatialLanguage, steels2008can}, pure statistical approaches through cross-situational learning \cite{xu2007word, smith2008infants} or more constrained statistical approaches \cite{roy2005semiotic, yu2007unified}. In all these existing models, meanings were expressed in terms of perceptual categories (e.g. in terms of shape, color, position, etc) \cite{steels06spatialLanguage, steels2008can,yu2007unified}, or in terms of motor actions \cite{steels2008robot, Massera2010,sugita05a}. This applies to models implemented in robots, such as in \cite{heckmann2009teaching}, where  the robot ASIMO is taught to associate new spoken signals to visual object properties, both in noisy conditions and without the need for bootstrapping. 

Only very few models so far have explored how other categories of word meanings could be learned. \cite{cederborg2011imitating} presented a model where word meaning expressed the cognitive operation of attentional focus, and some models of grammar acquisition dealt with the acquisition of grammatical markers which meaning operates on the disambiguation of other words in a sentence \cite{steels2012fluid}. 
The work we present in this article shows mechanisms allowing a learner to acquire word meaning associated to teaching and guidance in the context of social interaction. Furthermore, while in most models of lexicon  acquisition no tasks are learned (only word-meaning associations are learned), we show mechanisms allowing the learner to leverage learned word meanings to learn novel tasks from a human.

Finally, the work of Steels and colleagues \cite{steels2012grounding,steels2002aibos} have extensively shown the importance language games, instantiating various families of pre-programmed interaction protocols specifically designed to allow robots to learn speech sounds\cite{de2000self,oudeyer2006self}, lexicons \cite{steels2002aibos} or grammatical structures \cite{steels06spatialLanguage, steels2008can}. Other works used similar interactional frames to allow a structured interaction between humans and robots so that new elements of language could be identified and learnt by the robot learner \cite{roy02a,lyon2012interactive,cangelosi06b,yu2004multimodal,cangelosi2010integration,sugita05a,dominey2005learning,cederborg2011imitating}. In particular, it was shown that these interaction protocols fostered efficient language learning by implementing joint attention and joint intentional understanding between the robot and the human \cite{kaplan2006challenges,yu2005role,yu2007unified}, for example leveraging the synchronies and contingencies between the speech and the action flow \cite{rohlfing2006can,schillingmann2011acoustic}. Yet, to our knowledge, in all these existing works the interaction protocols have always been pre-programmed (that is the robot knows how to use and understand them innately, e.g. he knows how the teacher expresses ``good'' or ``bad'' feedback), as well as used by the robot learner to strictly identify new (form,meaning) pairs in the sensorimotor flow. The methods we present here form a basis on which such flexible and adaptive teaching protocols could be learnt.


Active Learning for Teaching a Robot Grounded Relational Symbols
Johannes Kulick and Marc Toussaint and
Tobias Lang
and
Manuel Lopes



\cite{spranger2013evolutionary} landmarks have an important function in spactial language. it can represent aocmmon ground, easier to refer to for each partners involved in the language game. it allow to evolve spatial relation. better with landmark than without. effect on success in communication and their impact on the
formation of spatial relations.
\cite{spranger2012emergent} \cite{spranger2013grounded} grounded acquisition experiments of increasing complexity. how various spatial language systems, such as projective, absolute and proximal can be learned. The proposed learning mechanisms do not rely on direct meaning transfer or direct access to world models of interlocutors.
\cite{spranger2012co} This chapter studies how basic spatial categories such as left-right, front-back, far-near or north-south can emerge in a population of robotic agents in co-evolution with terms that express these categories. It introduces various language strategies and tests them first in reconstructions of German spatial terms, then in acquisition experiments to demonstrate the adequacy of the strategy for learning these terms, and finally in language formation experiments showing how a spatial vocabulary and the concepts expressed by it can emerge in a population of embodied agents from scratch. Showed that the principles of selection and self-organization which were successfully used in earlier chapters to study the emergence of proper names, color terms and body postures could also be applied to study the emergence of basic spatial term

lingodroid \cite{schulz2010robots} : use iRats, have shared attention, when they encounter one antoher they communicate: where are we, what time of the day is it (use a wird and measure the light level), and meet-at a location at a specific time. The concept of morning, afternoon are changing with the season according to the lightning cycle. \cite{schulz2011lingodroids}
\cite{heath2012long} cocevolve geopersonal spacial language and language for time event, while building the map (no language a priori defined). base donly on the light level (simulated no noise) for example, day-night cycle. one year long period. gorunding words, concep of cyclic time throug experience wth events rather than by time or calendar.


difference with language games and our work: the realtion is direct, it applies to where we are now, the time is it now

\section{interactive and language}

\cite{branavan2011learning} This paper presents a novel approach for lever- aging automatically extracted textual knowl- edge to improve the performance of control applications such as games. Our ultimate goal is to enrich a stochastic player with high- level guidance expressed in text. Our model jointly learns to identify text that is relevant to a given game state in addition to learn- ing game strategies guided by the selected text. Our method operates in the Monte-Carlo search framework, and learns both text anal- ysis and game strategies based only on envi- ronment feedback. We apply our approach to the complex strategy game Civilization II using the official game manual as the text guide. Our results show that a linguistically-informed game-playing agent significantly outperforms its language-unaware counterpart

\cite{cederborg2011imitating} probablyt he closest work to this work, there is several trajectory, and several referential. by making hypothesis that each trajectory refer to each of the referential, we can find out which on belong to which referential and find which trajectories belong together. can reproduce a gesture wrt. an other combination of movement object. 
difference: the association related to the referential, the frame is direct reasoning in the agent head, the robot do not know how to act in the first place, learnthe gesture and generalize reproduction in other coordinate systems.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Unsupervised learning}


\subsection{Multimodal learning}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Ad hoc team}

As robots are moving into the real world, they will increasingly need to group together for cooperative activities with previously unknown teammates. In such ad hoc team settings, team strategies cannot be developed a priori. Rather, each robot must be prepared to cooperate with many types of teammates, which may not share the same capabilities or communicative means. 

This challenge of collaboration without pre-coordination has been first identified in the LARG group with the work on Ad Hoc Autonomous Agent Teams \cite{stone2010ad} where agents should learn to collaborate without defining pre-coordination scheme or knowing what the other agent will be capable of. Samuel Barrett, a last year PhD student, has been the main investigator of this line of research in the LARG group. He notably investigated complex teamwork domain such as the pursuit domain \cite{barrett2011empirical}, which is the domain we will use for our common project. The first development of the Ad Hoc Team project did not consider direct communication between agent \cite{stone2010ad} \cite{barrett2011adhoc} and the collaboration were effective only through the understanding of the behavior of the other agent \cite{barrett2011empirical} \cite{barrett2013team}. More recently, Samuel Barrett et al. introduced a minimal domain with communication and proved that ad hoc team agents can successfully cooperate with unknown teammates \cite{barrett2013communicating}. This latter work considers an abstract task using an armed bandit setting. Our joint collaboration aims at extending this work to the pursuit domain. Especially we would like to investigate a domain where communication between agents is mandatory to succeed in the task. 

]. And in the LARG Group, the focus was more on multiagent scenarios where robot must be prepared to cooperate with many types of teammates, which may not share the same capabilities or communicative means. The first phase of the Ad Hoc Team project considered indirect communication \cite{stone2010ad} \cite{barrett2011empirical} \cite{barrett2011adhoc} \cite{barrett2013team} and direct communication has been explored recently in an armed bandit scenario \cite{barrett2013communicating}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The human in the loop}
\label{chapter:related:humanintheloop}


As mentioned before, an important challenge for such interactive systems is to deal with non-expert humans whose teaching styles can vary considerably. Users may have various expectations and preferences when interacting with a robot and predefined protocols or instructions may bother the user and dramatically decrease the performance of the learning system \cite{rouanet2013impact}. 

Although learning from completely generic feedback types is very hard, we follow the approach from \cite{griffiths2012bottom}, which explores a human to human interaction in a categorization task where instruction can only be provided via six unlabeled symbols (thus the meaning of teaching signals are unknown to the learner). This study shows that tutors seem to spontaneously use three main types of instruction in order to help the learner: positive feedback, negative feedback, and concrete instructions (e.g. name of next optimal action). Yet, to our knowledge, nobody has investigated this problem with artificial agent in the context of human-robot interaction.


Yet, this capability is crucial in infant social development and learning, as well as in adult mutual adaptation of social cues. This has been the subject of experiments in experimental semiotics \cite{galantucci2009experimental}, such as in the work of Griffiths et al. \cite{griffiths2012bottom} who conducted an experiment with human learners learning the meaning of unknown symbolic teaching signals. The experimental setup we present here in the context of human-robot interaction is a variant of theirs, where teaching signals are sub-symbolic and not from a pre-determined set.

idea of transparency: the teacher should undertand what is undersood or not by the machine. What are its intention when acting ...
\cite{chao2010transparent} active learning imporve the accuracy and efficiency of the teaching process but may illicit undesirable effect of acceptabiliy reagridng the balance of the interaction. the balance of leader follower is not always maintained.

\cite{cakmak2012designing} using human-human experiment to find out which question are more often used. featue queries (explicit test on certain aspect of a task, can I do that?) are the most common in HRI and are perceived the smartest when used in robot. 

People will not always respect predefined conventions. Several studies discuss the different behaviors naive teachers use when instructing robots \cite{thomaz2008teachable,Cakmak2010optimality}. An important aspect is that the feedback is frequently ambiguous and deviates from the mathematical interpretation of a reward or a sample from a policy. For instance, in the work of \cite{thomaz2008teachable} the teachers frequently gave a positive reward for exploratory actions even if the signal was used by the learner as a standard reward. Also, even if we can define an optimal teaching sequence, humans do not necessarily behave according to those strategies \cite{Cakmak2010optimality}. are users optimal when teaching robots? not all al apspect even if asked to do so.

For more studies on how humans teach robots see \cite{thomaz2009learning,kaochar2011towards,knox2012humans}. These studies show that even when using well defined protocols, it is important to consider how different instructions can be used for learning. 



derived principle: \cite{thomaz2008teachable} 
transparency, balance of control leder follower
\cite{cakmak2010designing} led to conlusion about balance of autonomy and control. a question every step is boring,  and asking sometime is unpredictible. Letting the user send feedback when he wanted was prefered but less efficient.
\cite{knox2009design} human reinforcemnt function is a moving target. human reinforce almost always state action pairs and not state only. human trainers reinforce expected action (if the robot trun right in the direction of the goal, people reinforce, people see intentionality in the action) as well as recent actions. about to do or has just done. how to divide credits between future or past actions is not obvious yet.
\cite{kaochar2011towards} the ability for the teacher to test what is undertsood by the learner is important.



\cite{akgun12hri} kinesthetic teaching, demonstration by keyframe marking. the robot just have to find the keyframes to connect the keyframes.

\section{BCI}

Interestingly, non-invasive brain-computer interfaces (BCIs) have also looked at this problem. EEG-based BCIs have been used successfully to control different devices, such as robotic arms and simulated agents, using self-generated (e.g. motor imagery) and event-related potentials signals (see \cite{millan10} for a review). 
%
Error-related potential is one event-related potential that appears when the user's expectation diverges from the actual outcome. They have been recently used as reward to teach devices a policy that solves a user's intended task \cite{chavarriaga2010learning,iturrate2010robot}.

As in most BCI applications, it is necessary to perform a calibration phase to learn a decoder (e.g. a classifier) that detects this error-related potential in a single trial. This calibration is required mainly due to various characteristics of the EEG signals: the non-stationary nature \cite{vidaurre11}; the large intra- and inter-subject variability \cite{Polich1997}, and the variations induced by the task \cite{iturrate2013task}. This phase hinders the deployments of BCI applications out of the lab and calibration free methods have been identified as an important step to apply this technology in real applications \cite{millan10}. 
There are few BCI applications that are able to calibrate themselves during operation.  For long term operation using motor rhythms, it is possible to adapt the decoder online \cite{vidaurre2010towards}. For P300 spellers, Kindermans et al. proposed a method to autocalibrate the P300 detector by exploiting multiple stimulations and prior information \cite{Kindermans2012a,Kindermans2012b}. However, Kindermans et al, due to the specific interface used (P300Speller) can guarantee that 1 signal out of 6 has a specific meaning. In our work, and in a majority of sequential problems, it is impossible to define a sequence of actions that guarantee a specific ratio of meanings in the received signals.

\cite{kindermans2014integrating} they merge most of the method they have been developing. Dynamic stopping, where the system stop when it reaches a confidence threshold. Language model as a prior probability on next letter. Transfer learning where a model of previous subjects is used to ``regularizes the subject-specific solution towards the general model.''. Unsupervised learning (that is the EM approach we refer to when citing their work)

\cite{schettini2014self} they call themselves self-calibration but it isn't in the sense we are using it. It is actually continuous adaptation of the classifier model. They have a first session using a calibration procedure, and on latter run, the system re-calibrate itself continuously.

\todo{put the review paper here from chavariagga}

\section{stuff}

Here, we allow the robot to learn the meaning of unknown signals without the need of bootstrapping the system with known signals and by considering real natural speech waves data instead of symbolic labels, as well as a human-robot interaction scenario with a real robot. We extend the work presented in \cite{grizou2013robot} by defining an uncertainty measure allowing the agent to plan its actions efficiently which reduces the learning time.
